#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# This builtin function trains simple feed-forward neural network. The architecture of the
# networks is: affine1 -> relu -> dropout -> affine2 -> configurable output activation function.
# Hidden layer has 128 neurons. Dropout rate is 0.35. Input and output sizes are inferred from X and Y.
#
# INPUT:
# ------------------------------------------------------------------------------------------
# X                 Training data
# Y                 Labels/Target values
# batchSize         Batch size
# epochs            Number of epochs
# lr                Learning rate
# outActivation     User specified output activation function. Possible values:
#                   "sigmoid", "relu", "lrelu", "tanh", "softmax", "logits" (no activation).
# lossFn            User specified loss function. Possible values:
#                   "l1", "l2", "log_loss", "logcosh_loss", "cel" (cross-entropy loss).
# shuffle           Flag which indicates if dataset should be shuffled or not
# validationSplit   Fraction of training set used as validation set
# seed              Seed for model initialization
# verbose           Flag which indicates if function should print to stdout
# ------------------------------------------------------------------------------------------
#
# OUTPUT:
# -------------------------------------------------------------------------------
# model  Trained model which can be used in ffPredict
# -------------------------------------------------------------------------------

source("nn/layers/affine.dml") as affine
source("nn/layers/dropout.dml") as dropout
source("nn/layers/feedForward.dml") as ff_pass

# Supported loss functions by the model
source("nn/layers/l1_loss.dml") as l1_loss
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/log_loss.dml") as log_loss
source("nn/layers/logcosh_loss.dml") as logcosh_loss
source("nn/layers/cross_entropy_loss.dml") as cel

# Supported activation functions by the model
source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/relu.dml") as relu
source("nn/layers/leaky_relu.dml") as lrelu
source("nn/layers/tanh.dml") as tanh
source("nn/layers/softmax.dml") as softmax

source("nn/optim/sgd_nesterov.dml") as sgd_nesterov

m_ffTrain = function(Matrix[double] X, Matrix[double] Y, Integer batchSize=64,
  Integer epochs=20, Double lr=0.003, String outActivation,
  String lossFn, Boolean shuffle=FALSE, Double validationSplit = 0.0,
  Integer seed=-1, Boolean verbose=FALSE) 
  return (List[unknown] model)
{

  N = nrow(X) # number of samples
  D = ncol(X) # number of features
  t = ncol(Y) # number of targets

  if(shuffle) {
    [X, Y] = shuffle(X, Y)
  }

  validation = FALSE
  if(validationSplit > 0.0) {
    validation = TRUE
    [X_train, Y_train, X_val, Y_val] = val_split(X, Y, validationSplit)
    N = nrow(X_train)
  } else {
    X_train = X
    Y_train = Y
  }

  H1 = 128 # number of layer1 neurons

  # Init layers
  [W1, b1] = affine::init(D, H1, seed)
  [W2, b2] = affine::init(H1, t, seed)

  # Initialize SGD
  mu = 0 
  decay = 0.99 
  vW1 = sgd_nesterov::init(W1)
  vb1 = sgd_nesterov::init(b1)
  vW2 = sgd_nesterov::init(W2) 
  vb2 = sgd_nesterov::init(b2)

  iters = ceil(N / batchSize)

  batch = batchSize
  for (e in 1:epochs) {
    loss = 0
    val_loss = 0
    for(i in 1:iters) {
      
      begin = (i-1)*batch+1
      end = min(N, begin + batch - 1)
      X_batch = X_train[begin:end,]
      Y_batch = Y_train[begin:end,]
      
      # Output activation function is stored in the list together
      # with the layers since different activation functions might
      # be specified (or none). When forward/backward pass is 
      # performed, member "activation" of the list model is used 
      # such that appropriate functions are applied. This is
      # advantage for the user which does not have to pass model 
      # and activation function as two arguments in predict method.
      layers = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=outActivation)
      cache = ff_pass::feedForward(X=X_batch, layers=layers)

      # Distinguish two cases when loss is calculated from the raw output
      # or from the output of the activation function.
      if (outActivation != "logits") {
        loss = loss + loss_forward(as.matrix(cache["outs2"]), Y_batch, lossFn)
        dout2 = loss_backward(as.matrix(cache["outs2"]), Y_batch, lossFn)
      } else {
        loss =  loss + loss_forward(as.matrix(cache["out2"]), Y_batch, lossFn)
        dout2 = loss_backward(as.matrix(cache["out2"]), Y_batch, lossFn)
      }

      [dW1, db1, dW2, db2] = feed_backward(X_batch, layers, cache, dout2)

      [W2, vW2] = sgd_nesterov::update(W2, dW2, lr, mu, vW2)
      [b2, vb2] = sgd_nesterov::update(b2, db2, lr, mu, vb2)
      [W1, vW1] = sgd_nesterov::update(W1, dW1, lr, mu, vW1)
      [b1, vb1] = sgd_nesterov::update(b1, db1, lr, mu, vb1)

      if(validation) {
        cache = ff_pass::feedForward(X=X_val, layers=layers)
        if (outActivation != "logits")
          val_loss = val_loss + loss_forward(as.matrix(cache["outs2"]), Y_val, lossFn)
        else
          val_loss = val_loss + loss_forward(as.matrix(cache["out2"]), Y_val, lossFn)
      }
    }
    
    mu = mu + (0.999 - mu)/(1+epochs-e)
    lr = lr * decay

    if(validation) {
      print("Epoch: " + e + ", Train loss: " + loss/iters + " Validation loss: " + val_loss/iters)
    } else if(verbose) {
      print("Epoch: " + e + ", Train loss: " + loss/iters)
    }
  }
  model = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=outActivation)
}


feed_backward = function(Matrix[double] X, List[unknown] layers, List[unknown] cache, Matrix[double] dout) 
  return(Matrix[double] dW1, Matrix[double] db1, Matrix[double] dW2, Matrix[double] db2)
{
  p = 0.35 # dropout probability
  
  if (as.scalar(layers["activation"]) != "logits")
    dout = apply_activation_backward(dout, as.matrix(cache["out2"]), as.scalar(layers["activation"]))
  # Layer 2
  [doutd1, dW2, db2] = affine::backward(dout, as.matrix(cache["outd1"]), as.matrix(layers["W2"]), as.matrix(layers["b2"]))
  # Layer 1
  doutr1 = dropout::backward(doutd1, as.matrix(cache["outr1"]), p, as.matrix(cache["maskd1"]))
  dout1 = relu::backward(doutr1, as.matrix(cache["out1"]))
  [dx, dW1, db1] = affine::backward(dout1, X, as.matrix(layers["W1"]), as.matrix(layers["b1"]))
}

apply_activation_backward = function(Matrix[double] dout, Matrix[double] X, String activation) 
  return (Matrix[double] out)
{
  if(activation == "sigmoid") {
    out = sigmoid::backward(dout, X)
  } else if (activation == "relu") {
    out = relu::backward(dout, X)
  } else if (activation == "lrelu") {
    out = lrelu::backward(dout, X)
  } else if (activation == "tanh") {
    out = tanh::backward(dout, X)
  } else if (activation == "softmax") {
    out = softmax::backward(dout, X)
  }
}

loss_forward = function(Matrix[double] prediction, Matrix[double] target, String lossFn)
  return(Double loss)
{
  if (lossFn == "l1") {
    loss = l1_loss::forward(prediction, target)
  } else if(lossFn == "l2") {
    loss = l2_loss::forward(prediction, target)
  } else if(lossFn == "log_loss") {
    loss = log_loss::forward(prediction, target)
  } else if(lossFn == "logcosh_loss") {
    loss = logcosh_loss::forward(prediction, target)
  } else {
    loss = cel::forward(prediction, target)
  }
}

loss_backward = function(Matrix[double] prediction, Matrix[double] target, String lossFn)
  return(Matrix[Double] dout)
{
  if (lossFn == "l1") {
    dout = l1_loss::backward(prediction, target)
  } else if(lossFn == "l2") {
    dout = l2_loss::backward(prediction, target)
  } else if(lossFn == "log_loss") {
    dout = log_loss::backward(prediction, target)
  } else if(lossFn == "logcosh_loss") {
    dout = logcosh_loss::backward(prediction, target)
  } else {
    dout = cel::backward(prediction, target)
  }
}

shuffle = function(Matrix[double] X, Matrix[double] Y)
  return(Matrix[Double] X_new, Matrix[Double] Y_new)
{
  X_col = ncol(X)
  Y_col = ncol(Y)
  ord = rand(rows=nrow(X), cols=1, min=0, max=1, pdf="uniform")
  shuffled = order(target = cbind(X, Y, ord), by = X_col + Y_col + 1)

  X_new = shuffled[,1:X_col]
  Y_new = shuffled[,X_col + 1 : X_col + Y_col]
}

val_split = function(Matrix[double] X, Matrix[double] Y, Double split)
  return(Matrix[double] X_train, Matrix[double] Y_train,
  Matrix[double] X_val, Matrix[double] Y_val)
{
  N = nrow(X)
  val_start = round(N * split)
  X_train = X[1:N - val_start,]
  Y_train = Y[1:N - val_start,]
  X_val = X[N - val_start + 1:N,]
  Y_val = Y[N - val_start + 1:N,]
}
