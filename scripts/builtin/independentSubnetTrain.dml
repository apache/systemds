m_independentSubnetTrain = function(
   list[unknown] model,
   matrix[double]       features,
   matrix[double]       labels,
   matrix[double]       val_features,
   matrix[double]       val_labels,
   string               upd,
   string               agg,
   string               mode,
   string               utype,
   int                  epochs,
   int                  batchsize,
   int                  j,
   int                  k,
   string               scheme,
   list[unknown]        hyperparams,
   boolean              verbose,
   int                  paramsPerLayer,
   list[int]          fullyConnectedLayers
)
return (list[unknown] model_out_2)
{
    # ------------------------------------------------------------
    # Setup
    # ------------------------------------------------------------
    print("Entered IST function.")
    model_out = model

    P = length(model)
    print("Parameters in model:")
    print(P)

    N = nrow(features)
    print("Samples:")
    print(N)

    print("Is model length NOT divisible by paramsPerLayer? :")
    print(P %% paramsPerLayer != 0)
    if (P %% paramsPerLayer != 0) {
       stop("Model length not divisible by paramsPerLayer")
    }

    print("We made it")
    L = as.integer(P / paramsPerLayer)  # total layers
    print("Layers:")
    print(L)

    # obtain indices of FC layers
    fcLayers = fullyConnectedLayers
    print("FC layers:")
    print(toString(fcLayers))

    # I. determine shared parameters
    isSharedParam = matrix(0, 1, P)

    # create mask for all parameters of FC layers
    isFC = matrix(0, rows=1, cols=L)
    for (i in 1:length(fcLayers)) {
       idx = as.integer(as.scalar(fcLayers[i]))
       isFC[1, idx] = 1
    }
    print(toString(isFC))

    isFC_rep = isFC
    for (r in 2:paramsPerLayer) {
        isFC_rep = cbind(isFC_rep, isFC)
    }
    print(toString(isFC_rep))
    if (ncol(isFC_rep)!=P) stop("Dimension mismatch for FC layer mask.")


    # 1. all non-FC layers are shared
    isSharedParam = 1 - isFC_rep
    print(toString(isSharedParam))


    # 2. FC bias parameters are shared in: output layer or at the end of a FC block
    for (paramId in seq(2, paramsPerLayer, 2)) {   # iterate bias blocks only
        for (l in 1:L) {
            if (as.scalar(isFC[1,l])==1 & l==L) {
                p_out_bias = (paramId - 1) * L + L  # output bias is shared across subnets
                isSharedParam[1, p_out_bias] = 1
            }
            else if (as.scalar(isFC[1,l])==1 & l<L & as.scalar(isFC[1,l+1])==0) {
                p_out_bias = (paramId - 1) * L + l  # end of FC block's bias is shared across subnets
                isSharedParam[1, p_out_bias] = 1
            }
        }
    }
    print(toString(isSharedParam))
    if (ncol(isSharedParam) != P) stop("isSharedParam dimension mismatch")

    # II. calculate update-steps per epoch
    if (batchsize <= 0) {
        stepsPerEpoch = 1  # full batch at once TODO what happens in training loop then? use allSampleIndicesRandom directly as batchIndex??
    } else {
        stepsPerEpoch = ceil(N / batchsize)
    }
    print("Steps per epoch:")
    print(stepsPerEpoch)


    # III. training loop
    for (epoch in 1:epochs) {
        # reshuffle indices each epoch
        randyOrton = rand(rows=N, cols=1)
        allSampleIndicesRandom = order(target=randyOrton, by=1, decreasing=FALSE, index.return=TRUE)
        print("Length of random sample indices:")
        print(length(allSampleIndicesRandom))
        # print(toString(allSampleIndicesRandom))

        # iterate IST rounds
        for (step in seq(1, stepsPerEpoch, j)) {
            print("Iterating IST round:")
            print(step)
            # TODO tested until here! - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

            # 1.) create masks for all subnets
            masks = ist_create_disjoint_masks(model_out, k, L, fcLayers, paramsPerLayer, isFC)
            #print(toString(masks[1,1]))
        }
    }

    # TODO leave as return statement - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    model_out_2 = model
}


# ------------------------------------------------------------
# Independent Subnet Masking
#
# This helper function creates a list of masks, one per subnet.
# Each mask is a binary vector/matrix indicating which parameters belong to that subnet.
# ------------------------------------------------------------
# ASSUMPTIONS:
#  - neuron ownership is defined via bias vectors
#  - model is a list of parameter tensors
#  - trainable parameters are grouped by parameter type i.e. param blocks like (W_l1, W_l2, ..., b_l1, b_l2, ...)
#  - assumes W and b are always the first two param blocks
#  - the pattern of optional optimizer state tensors (e.g., vW_l, vb_l) follow the same grouping and always W followed by b
#  - (output layer & end of FC block) biases are shared -> gradients collide; must be handled by aggregation logic
# ------------------------------------------------------------

ist_create_disjoint_masks = function(
    list[unknown] model,
    int numSubnets,
    int L,  # total layers including output layer
    list[int] fullyConnectedLayers,  # the indices of FC-layers starting from 1
    int paramsPerFCLayer,
    Matrix[Double] isFC)
  return (list[unknown] masks)
{
    P = length(model)
    modus = 0  # {0: matrix / 1: list}

    # SANITY CHECKS: ensure provided model can be masked correctly
    if (as.integer(P / paramsPerFCLayer) != L) {
        stop("Layer/parameter mismatch. Please make sure each layer has the same amount of parameters.")
    };
    if (paramsPerFCLayer < 2 | paramsPerFCLayer %% 2 != 0) {
        stop("At least 1 pair of W and b needs to be present, as well as parameters need to be W&b pairs.")
    }

    # I.) initialize and preallocate masks
    masks = list()
    for (s in 1:numSubnets) {
        masks = append(masks, model);
        #for (k in 1:P) { TODO might be a problem?
        #    masks[s][k] = matrix(0, rows=nrow(model[p]), cols=ncol(model[p]))
        #}
    }
    print("Masks now has following length:")
    print(length(masks))


    # II.) determine FC layers
    print("Forwarded fully connected layer information:")
    print(toString(isFC))

    # TODO NEW START - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - -

    masks_new_meta = matrix(0, rows=length(model), cols=4)  # columns=[start,end,rows,cols]
    current_position = 1
    for (p in 1:length(model)) {
        M = as.matrix(model[p])
        param_length = ncol(M) * nrow(M)  # as.scalar(ncol(M)) * as.scalar(nrow(M))

        masks_new_meta[p,1] = current_position
        masks_new_meta[p,2] = current_position + param_length -1
        masks_new_meta[p,3] = nrow(M)
        masks_new_meta[p,4] = ncol(M)

        current_position = current_position + param_length
    }
    mask_size = current_position-1

    # All subnets in one matrix
    masks_new = matrix(0, rows=numSubnets, cols=mask_size)


    print(masks_new_meta)

    # TODO NEW END - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - -


    # III.) iterate all layers
    for (l in 1:L) {
        if (as.scalar(isFC[1,l]) == 1) {
            print("Entered fully connected layer. The layer is:")
            print(l)

            W = as.matrix(model[l])
            print("W (rows/cols):")
            print(nrow(W))
            print(ncol(W))

            b = as.matrix(model[l+L])
            print("b:")
            print(ncol(b))

            H = ncol(W);  # bias neurons in layer l
            print("H:")
            print(H)

            # SANITY CHECK
            if (nrow(b) != 1 | ncol(b) != H) {
                print("Bias shape mismatch!")
                print("b:", nrow(b), "x", ncol(b))
                print("expected: 1 x", H)
                stop("Invalid bias shape")
            }
            if (l!=L & numSubnets>ncol(b)) {  # TODO change to next layer is non-FC logic
                print("More subnets than available neurons in layer:")
                print(l)
                stop("Please use a wider model or decrease the amount of subnets.")
            }

            randyOrton2 = rand(rows=H, cols=1)  # shuffle all indices
            allNeuronIndicesRandom = order(target=randyOrton2, by=1, decreasing=FALSE, index.return=TRUE)  # shuffle all indices
            print("Length of random sample indices:")
            print(length(allNeuronIndicesRandom))

            # TODO FROM HERE ...
            chunk_size = floor(H/numSubnets)  # amount of neurons each subnet will consist at least TODO for l=L they are shared s this value will be quite low or even below 1
            remaining_neurons = H - chunk_size * numSubnets
            print("Dividing all hidden layer neurons by the number of subnets, each subnet will own at least:")
            print(chunk_size)
            print("Following amount of neurons remains and will be randomly assigned to the subnets (at most one to a subnet):")
            print(remaining_neurons)

            amount_active_neurons = matrix(chunk_size, rows=numSubnets, cols=1)
            if (remaining_neurons > 0) {
                randomSubnetIndices = order(target=rand(rows=numSubnets, cols=1, seed=-1), by=1, decreasing=FALSE, index.return=TRUE)  # TODO replace seed for experiments
                for (i in 1:remaining_neurons) {
                  sid = as.integer(as.scalar(randomSubnetIndices[i,1]))
                  amount_active_neurons[sid,1] = as.scalar(amount_active_neurons[sid,1]) + 1  # TODO use pmin()
                }
            }
            print("Amount of active neurons per subnet:")
            print(amount_active_neurons)

            neuron_end_indices = cumsum(amount_active_neurons)
            neuron_start_indices = neuron_end_indices - amount_active_neurons + 1
            print("Start indices of each subnet:")
            print(neuron_start_indices)

            for(s in 1:numSubnets) {
                print("Entered subnet:")
                print(s)

                # A. obtain owned neurons for this layer
                start = as.integer(as.scalar(neuron_start_indices[s,1]))
                end = as.integer(as.scalar(neuron_end_indices[s,1]))
                current_b_indices = allNeuronIndicesRandom[start:end, 1]
                print(length(current_b_indices))
                # TODO ... UNTIL HERE: only required for else case (in bias case)

                # B. create masked bias
                if(l==L) {  # output layer
                    masked_b = matrix(1, rows=1, cols=ncol(b))
                }
                else if (l<L & as.scalar(isFC[1, l+1]) == 0) {  # next layer is not FC TODO works with || operator?
                    masked_b = matrix(1, rows=1, cols=ncol(b))
                }
                else {
                    masked_b = matrix(0, rows=1, cols=ncol(b))
                    for (i in 1:nrow(current_b_indices)) {  # TODO vectorized possible?
                       idx = as.integer(as.scalar(current_b_indices[i,1]))
                       masked_b[1, idx] = 1
                    }
                }
                #print(masked_b)

                # 2b. create masked weight
                masked_W = matrix(0, rows=nrow(W), cols=ncol(W))
                if(l==1) {
                    for (i in 1:nrow(current_b_indices)) {  # TODO vectorized possible?
                       idx = as.integer(as.scalar(current_b_indices[i,1]))
                       masked_W[1:nrow(W), idx] = matrix(1, rows=nrow(W), cols=1)
                    }
                }
                else if (l>1 & as.scalar(isFC[1, l-1])==0) {  # previous layer is not FC TODO works with || operator?
                    for (i in 1:nrow(current_b_indices)) {  # TODO vectorized possible?
                       idx = as.integer(as.scalar(current_b_indices[i,1]))
                       masked_W[1:nrow(W), idx] = matrix(1, rows=nrow(W), cols=1)
                    }
                }
                else {
                    # obtain active neurons of previous layer
                    p = L + (l-1) # TODO investigate

                    if (modus==1) {
                        previous_masked_b_list = as.list(masks[s])
                        previous_masked_b = as.matrix(previous_masked_b_list[p])
                        print(toString(previous_masked_b))
                    } else {
                        start = as.integer(as.scalar(masks_new_meta[p,1]))
                        end   = as.integer(as.scalar(masks_new_meta[p,2]))
                        r     = as.integer(as.scalar(masks_new_meta[p,3]))
                        c     = as.integer(as.scalar(masks_new_meta[p,4]))
                        print("Following info was obtained.")
                        print(start)
                        print(end)
                        print(r)
                        print(c)

                        vec = masks_new[s, start:end]
                        previous_masked_b = matrix(vec, rows=r, cols=c, byrow=TRUE)  # TODO might be unnecessary
                        print("Previous bias:")
                        print(previous_masked_b)
                    }

                    # SANITY CHECK: dimensions with layers of previous layer match
                    if (l > 1 & ncol(previous_masked_b) != nrow(W)) {
                        print("W/prev layer mismatch in layer l=", l)
                        print("prev_b:", nrow(previous_masked_b), "x", ncol(previous_masked_b))
                        print("W:", nrow(W), "x", ncol(W))
                        stop("Invalid W shape wrt previous layer")
                    }

                    if (nrow(previous_masked_b)==1) previous_masked_b = t(previous_masked_b)
                    if (ncol(masked_b) == 1) masked_b = t(masked_b)

                    if(l==L) {  # output layer
                        masked_W = previous_masked_b %*% matrix(1, 1, ncol(masked_W))
                    }
                    else if (l<L & as.scalar(isFC[1, l+1]) == 0) {  # next layer is not FC TODO works with || operator?
                        masked_W = previous_masked_b %*% matrix(1, 1, ncol(masked_W))
                    } else {
                        masked_W = previous_masked_b %*% masked_b
                    }
                    print("Previous b:")
                    print(previous_masked_b)
                }
                print("Current b:")
                print(masked_b)
                print("Current W:")
                print(masked_W)
                # TODO tested until here! - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
                # 3. forward these masks to all parameters in this layer
                if (modus==1) {
                    # TODO this branch is faulty ATM
                    for (param in 1:paramsPerFCLayer) {
                        k = (param-1)*L + l

                        if (param %% 2 == 0) {
                            # masks[s][k] = masked_b FIXME
                        } else {
                            # masks[s][k] = masked_W FIXME
                        }
                    }
                    #print(masks[s])
                    # TODO this branch is faulty ATM
                } else {
                    for (param in 1:paramsPerFCLayer) {
                        k = (param-1)*L + l
                        start = as.integer(as.scalar(masks_new_meta[k,1]))
                        end   = as.integer(as.scalar(masks_new_meta[k,2]))
                        len   = end - start + 1

                        if (param %% 2 == 0) {
                            #masks_new[s,start:end] = masked_b
                            flat = matrix(masked_b, rows=1, cols=len, byrow=TRUE)
                        } else {
                            flat = matrix(masked_W, rows=1, cols=len, byrow=TRUE)
                        }
                        masks_new[s, start:end] = flat
                    }
                    print("Saved masks for this layer:")
                    print(masks_new[s])
                }
            }

            # SANITY CHECK: accumulating all subnets bias's result in vector of all 1s (in FC hidden layers only)
            if (l < L) {
                if (as.scalar(isFC[1, l+1]) == 1) {
                    # bias parameter index for layer l is (L + l)
                    p = L + l

                    start = as.integer(as.scalar(masks_new_meta[p,1]));
                    end   = as.integer(as.scalar(masks_new_meta[p,2]));
                    r     = as.integer(as.scalar(masks_new_meta[p,3]));
                    c     = as.integer(as.scalar(masks_new_meta[p,4]));

                    # sum across subnets for this bias slice
                    sumB_flat = colSums(masks_new[1:numSubnets, start:end])

                    # reshape back to bias shape (usually Hx1 or 1xH depending on how you store it)
                    sumB = matrix(sumB_flat, rows=r, cols=c, byrow=TRUE)

                    if (min(sumB) != 1 | max(sumB) != 1) {
                        print("Subnet bias masks not a partition in layer l=" + l)
                        print("min(sumB)=" + min(sumB) + " max(sumB)=" + max(sumB))
                        stop("Invalid subnet bias partition")
                    }
                }
            }
        }
        else {
            # independent subnet training cannot be used on this layer // Non-FC layer: shared across all subnets -> masks are all-ones
            for (param in 1:paramsPerFCLayer) {
                k = (param-1)*L + l
                start = as.integer(as.scalar(masks_new_meta[k,1]))
                end   = as.integer(as.scalar(masks_new_meta[k,2]))
                r     = as.integer(as.scalar(masks_new_meta[k,3]))
                c     = as.integer(as.scalar(masks_new_meta[k,4]))
                len   = end - start + 1

                if (param %% 2 == 0) {
                    shared_b = matrix(1, rows=r, cols=c)
                    flat = matrix(shared_b, rows=1, cols=len, byrow=TRUE)
                } else {
                    shared_W = matrix(1, rows=r, cols=c)
                    flat = matrix(shared_W, rows=1, cols=len, byrow=TRUE)
                }
                masks_new[1:numSubnets, start:end] = matrix(1, numSubnets, 1) %*% flat
            }
        }
    }
    print("This is the final mask for the 1st subnet:")
    print(masks_new[1])
    s = 1
    # TODO tested until here! - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    for (p in 1:paramsPerFCLayer) {
        start = as.integer(as.scalar(masks_new_meta[p,1]))
        end   = as.integer(as.scalar(masks_new_meta[p,2]))
        r     = as.integer(as.scalar(masks_new_meta[p,3]))
        c     = as.integer(as.scalar(masks_new_meta[p,4]))
        print("Following info was obtained.")
        print(start)
        print(end)
        print(r)
        print(c)

        vec = masks_new[s, start:end]
        previous_masked_b = matrix(vec, rows=r, cols=c, byrow=TRUE)
        print(previous_masked_b)
    }
    stop("Execution stopped.")
    # TODO leave as return statement - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    masks = model
}