#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("nn/layers/attention.dml") as attention
source("nn/layers/affine.dml") as affine
source("nn/layers/lstm.dml") as lstm
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax
source("nn/optim/adam.dml") as adam
source("nn/layers/log_loss.dml") as log_loss


# 1 get data
data_loc = "scripts/nn/examples/data/"
tableschema = "string,string,fp32"
N=5000
n=100
max_length = 29
data = read(data_loc + "data.csv", format="csv", header=TRUE, sep=";", data_type="frame", schema=tableschema, cols=3, rows=N)
data = data[1:n]

# 2 prepare feature matrix for lstm into one-hot-encoded sequences
#spec = "{\"algo\" : \"ngram\",\"algo_params\": {\"min_gram\": 1, \"max_gram\": 3}, \"out\": \"count\", \"tokenize_col\": 2, \"id_cols\": [1,3]}"
spec = "{\"algo\" : \"split\", \"out\": \"position\", \"tokenize_col\": 2, \"id_cols\": [1], \"format_wide\": false, \"num_features\": 1}"
#print(toString(data[,1:2]))
#print(toString(data[1,2:3]))
X = tokenize(target=data[,1:2], spec=spec, max_tokens=30)
#print(toString(X))
#print(X[1])
recode_spec = "{ \"recode\": [C3]}"
[O, M] = transformencode(target=X, spec=recode_spec)
#print(toString(M))
#print(toString(O))

num_classes = max(O[,3])
#print(num_classes)
A = toOneHot(O[,3], num_classes)
#Features = cbind(O[,1:2], A)
Features = matrix(0, rows=n, cols=num_classes * max_length)
row_old = as.scalar(O[1,1])
pos = 1
R = nrow(O)
for(i in 1:R)
{
  row = as.scalar(O[i,1])
  #print(toString(row))
  if (row != row_old)
  {
    row_old = row
    pos = 1
  }
  Features[row,(pos-1)*num_classes+1:pos*num_classes] = A[i]
  pos += 1
}

#print(toString(nrow(Features)) + "x" + toString(ncol(Features)))
#print(toString(Features[1:20,1:20]))
#print(max(Features[,6]))

# 3 prepare target matrix to one-hot-encoded
target = as.matrix(data[,3]) + 1
target_classes = max(target)
Y = toOneHot(target, target_classes)
#print(toString(target[1:10]))
#print(toString(Y[1:10]))

# 4 train network
train(Features, Y, 10, 8, max_length)

train = function( matrix[double] x_train,
                  matrix[double] y_train,
                  integer epochs,
                  integer batch_size,
                  integer max_sequence_length)
  return(List[unknown] biases, List[unknown] weights)
{
  samples = nrow(x_train)
  features = ncol(x_train)
  token_size = features/max_sequence_length
  output_size = ncol(y_train)
  biases = list(samples)
  weights = list(samples)

  # 1 lstm layer
  lstm_neurons = 20
  [W_0, b_0, out0, c0] = lstm::init(batch_size, token_size, lstm_neurons)

  # 2 attention layer: no weights

  # 3 dense layer -> (hidden_size)
  hidden_neurons = 128

  [W_1, b_1] = affine::init(max_sequence_length * lstm_neurons, hidden_neurons, -1)

  # 4 dense layer -> (output_size)
  [W_2, b_2] = affine::init(hidden_neurons, output_size, -1)

  # 5 softmax layer: no weights

  # put weights & biases into list
  biases = list(b_0, b_1, b_2)
  weights = list(W_0, W_1, W_2)

  #optimizer init
  [mout0, vout0] = adam::init(out0)
  [mc0, vc0] = adam::init(c0)

  [mW_0, vW_0] = adam::init(W_0)
  [mW_1, vW_1] = adam::init(W_1)
  [mW_2, vW_2] = adam::init(W_2)

  [mb_0, vb_0] = adam::init(b_0)
  [mb_1, vb_1] = adam::init(b_1)
  [mb_2, vb_2] = adam::init(b_2)

  #optimizer params
  lr = 0.001
  beta1 = 0.99
  beta2 = 0.999
  epsilon = 1e-8
  t = 0

  #training loop
  iters = ceil(samples/batch_size)
  for (ep in 1:epochs)
  {
    for (i in 1:iters)
    {
      # 1 Get batch data
      start = ((i-1) * batch_size) %% samples + 1
      end = min(samples, start + batch_size -1)
      #TODO fix batch size problem

      x_batch = x_train[start:end,]
      y_batch = y_train[start:end,]

      # 2 predict
      [y_hat, out5, out4, out3, out2, out1, cache_out_out, cache_c_out, cache_ifog_out] =
          predict(x_batch, biases, weights, max_sequence_length, token_size, out0, c0)

      # 3 backpropagation

      dout = log_loss::backward(y_hat, y_batch)
      dprobs = softmax::backward(dout, out5)
      [dout_2, dW_2, db_2] = affine::backward(dprobs, out4, W_2, b_2)
      drelu = relu::backward(dout_2, out3)
      [dout_1, dW_1, db_1] = affine::backward(drelu, out2, W_1, b_1)
      datt = attention::backward(dout_1, out1, out1, out1, max_sequence_length)
      dc = matrix(0, rows=nrow(x_batch), cols=lstm_neurons)
      [dout_0, dW_0, db_0, dout0, dc0] = lstm::backward(datt,
                                                        dc,
                                                        x_batch,
                                                        W_0,
                                                        b_0,
                                                        max_sequence_length,
                                                        token_size,
                                                        TRUE,
                                                        out0,
                                                        c0,
                                                        cache_out_out,
                                                        cache_c_out,
                                                        cache_ifog_out)

      # 4 update weights & biases

      t = ep * i - 1
      # lstm
      [c0 , mc0 , vc0 ] = adam::update(c0 , dc0 , lr, beta1, beta2, epsilon, t, mc0 , vc0 )
      [out0,mout0,vout0]= adam::update(out0,dout0,lr, beta1, beta2, epsilon, t,mout0,vout0)
      [b_0, mb_0, vb_0] = adam::update(b_0, db_0, lr, beta1, beta2, epsilon, t, mb_0, vb_0)
      [W_0, mW_0, vW_0] = adam::update(W_0, dW_0, lr, beta1, beta2, epsilon, t, mW_0, vW_0)

      #first affine
      [b_1, mb_1, vb_1] = adam::update(b_1, db_1, lr, beta1, beta2, epsilon, t, mb_1, vb_1)
      [W_1, mW_1, vW_1] = adam::update(W_1, dW_1, lr, beta1, beta2, epsilon, t, mW_1, vW_1)

      #second affine
      [b_2, mb_2, vb_2] = adam::update(b_2, db_2, lr, beta2, beta2, epsilon, t, mb_2, vb_2)
      [W_2, mW_2, vW_2] = adam::update(W_2, dW_2, lr, beta2, beta2, epsilon, t, mW_2, vW_2)

      # put weights & biases into list
      biases = list(b_0, b_1, b_2)
      weights = list(W_0, W_1, W_2)

    }
    #print("Epoch: " + e + "; Train Loss: " + loss + "; Train Accuracy: " + accuracy)
  }
}

predict = function( matrix[double] x,
                    List[unknown] biases,
                    List[unknown] weights,
                    integer max_sequence_length,
                    integer token_size,
                    matrix[double] out0,
                    matrix[double] c0)
  return (matrix[double] y_hat, matrix[double] out5, matrix[double] out4, matrix[double] out3,
          matrix[double] out2, matrix[double] out1, matrix[double] cache_out_out,
          matrix[double] cache_c_out, matrix[double] cache_ifog_out)
{
  # unpack weights & biases
  W_0 = as.matrix(weights[1])
  W_1 = as.matrix(weights[2])
  W_2 = as.matrix(weights[3])

  b_0 = as.matrix(biases[1])
  b_1 = as.matrix(biases[2])
  b_2 = as.matrix(biases[3])

  # put input through layers
  [out1, c_out, cache_out_out, cache_c_out, cache_ifog_out]=
    lstm::forward(x, W_0, b_0, max_sequence_length, token_size, TRUE, out0, c0)
  out2 = attention::forward(out1, out1, out1, max_sequence_length)
  out2 = out1
  out3 = affine::forward(out1, W_1, b_1)
  out4 = relu::forward(out3)
  out5 = affine::forward(out4, W_2, b_2)
  y_hat = softmax::forward(out5)
}
