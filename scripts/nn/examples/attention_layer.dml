#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
source("nn/layers/attention.dml") as attention
source("nn/layers/conv1d.dml") as conv1d

#toy gradient example

M = 3
N = 2
L = 3
Q = matrix("0.1 0.1 0.2 0.2 0.3 0.3", rows=M, cols=N)
K = matrix("0.4 0.7 0.5 0.8 0.6 0.9", rows=M, cols=N)
V = matrix("0.13 0.161 0.19 0.14 0.17 0.2 0.15 0.18 0.21", rows=M, cols=L)

print("Q:\n" + toString(Q))
print("K:\n" + toString(K))
print("V:\n" + toString(V))

att = attention::forward(Q, K, V)

datt = 1
[dQ, dK, dV] = attention::backward(datt, Q, K, V)

print("dQ")
print(toString(dQ))
print("dK")
print(toString(dK))
print("dV")
print(toString(dV))

# ============== Simple test case =============================
# Create training data
q = matrix(1.1, rows=1, cols=1)
k = matrix(1.6, rows=1, cols=1)
v = matrix(1, rows=1, cols=1)

calculated = attention::forward(q, k, v)
# expected = q * k = 1.1 * 1.6 = 1.76, we can see that in the print of calculate_scores function, softmax for the full
#   funtionality of the attention layer.
expected = softmax(matrix(1.76, rows=1, cols=1))
assert(toString(calculated) == toString(expected))

# ============== Simple Example ======================================
# For simplicity and since systemDS has no embedding at the time of the
#   development of this test, we simply used numbers from
#   https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767.
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers from the link above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" x 4(d_models)
input_embeddings = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)
print("Input embeddings:\n" + toString(input_embeddings))
# initial weight matrices with 4 (d_models) x 3(d_k /d_v)
q_weights = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
k_weights = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
v_weights = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)
print("q_weights:\n" + toString(q_weights))
print("k_weights:\n" + toString(k_weights))
print("v_weights:\n" + toString(v_weights))

# 3. attention layer
query = input_embeddings %*% q_weights
key = input_embeddings %*% k_weights
value = input_embeddings %*% v_weights

attention = attention::forward(query, key, value)
print("attention: \n" + toString(attention))
# Results in scores:
#   0.136 0.432 0.432
#   0.001 0.909 0.090
#   0.007 0.755 0.238

# Attention vectors:
#   1.864 6.319 1.704
#   1.999 7.814 0.273
#   1.993 7.480 0.736

# Which is roughly the same result as in the article.

# ================ Practical example =================
# Similar to the example from https://keras.io/api/layers/attention_layers/attention/
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers from the link as in the example above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" x 4(d_models)
input_embeddings = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)
# initial weight matrices with 4 (d_models) x 3(d_k /d_v)
q_weights = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
k_weights = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
v_weights = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)

# 3. cnn layer
# foo = conv1d::forward(q_weights, 100, 0, 0, )
# print(foo)

# 5. dense layer



