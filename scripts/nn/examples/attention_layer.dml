#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
source("nn/layers/attention.dml") as attention
source("nn/layers/conv1d.dml") as conv1d
source("/nn/layers/avg_pool2d_builtin.dml") as avg_pool2d


# =================================== Simple test case ===================================
# Create training data
q = matrix(1.1, rows=1, cols=1)
k = matrix(1.6, rows=1, cols=1)
v = matrix(1, rows=1, cols=1)

calculated = attention::forward(q, k, v)
# expected = q * k = 1.1 * 1.6 = 1.76, we can see that in the print of calculate_scores function, softmax for the full
#   funtionality of the attention layer.
expected = softmax(matrix(1.76, rows=1, cols=1))
assert(toString(calculated) == toString(expected))


# =================================== Simple Example ===================================
print("================ Simple Example =================")
# For simplicity and since systemDS has no embedding at the time of the
#   development of this use case, we simply used numbers from
#   https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767.
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers from the link above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" => 3 x 4(d_models)
input_embeddings = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)

# initial weight matrices with 4 (d_models) x 3
q_weights = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
k_weights = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
v_weights = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)

# 3. attention layer
query = input_embeddings %*% q_weights
key = input_embeddings %*% k_weights
value = input_embeddings %*% v_weights

attention = attention::forward(query, key, value)
# The generated results are roughly the same result as in the article.
print("attention: \n" + toString(attention))


# =================================== Practical example ===================================
print("================ Practical example =================")
# Similar to the example from https://keras.io/api/layers/attention_layers/attention/
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers as in the example above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" x 4 (d_models)
token_embedding = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)
# initial weight matrices with 4 (d_models) x 3
query_embeddings = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
key_embeddings = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
value_embeddings = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)

# 3. cnn layer
filter = matrix("0.4 0.2 0.4", rows=1, cols=3) # TODO fine this way?
query_seq_encoding = conv1d::forward(query_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)
key_seq_encoding = conv1d::forward(key_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)
value_seq_encoding = conv1d::forward(value_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)

# 4. attention layer
attention_seq = attention::forward(query_seq_encoding, key_seq_encoding, value_seq_encoding)

# 5. Global Average Pooling
[query_encoding, Hout, Wout] = avg_pool2d::forward(t(query_seq_encoding), 1, 1, 4, 1, 2, 1, 1, 0, 0)
[attention, Hout, Wout] = avg_pool2d::forward(t(attention_seq), 1, 1, 4, 1, 2, 1, 1, 0, 0)

# 6. concatenate query and encodings, resulting in a DNN input layer.
input_layer = t(cbind(query_encoding, attention))
print("input layer: \n" + toString(input_layer))

