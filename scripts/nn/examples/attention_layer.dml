#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
source("nn/layers/attention.dml") as attention
source("nn/layers/conv1d.dml") as conv1d
source("/nn/layers/avg_pool2d_builtin.dml") as avg_pool2d

#toy gradient example

M = 3
N = 2
L = 3
Q = matrix("0.1 0.1 0.2 0.2 0.3 0.3", rows=M, cols=N)
K = matrix("0.4 0.7 0.5 0.8 0.6 0.9", rows=M, cols=N)
V = matrix("0.13 0.161 0.19 0.14 0.17 0.2 0.15 0.18 0.21", rows=M, cols=L)

print("Q:\n" + toString(Q))
print("K:\n" + toString(K))
print("V:\n" + toString(V))

att = attention::forward(Q, K, V)

# =========== TODO it is bugging here ===========
# datt = 1
# [dQ, dK, dV] = attention::backward(datt, Q, K, V)
#
# print("dQ")
# print(toString(dQ))
# print("dK")
# print(toString(dK))
# print("dV")
# print(toString(dV))

# ============== Simple test case =============================
# Create training data
q = matrix(1.1, rows=1, cols=1)
k = matrix(1.6, rows=1, cols=1)
v = matrix(1, rows=1, cols=1)

calculated = attention::forward(q, k, v)
# expected = q * k = 1.1 * 1.6 = 1.76, we can see that in the print of calculate_scores function, softmax for the full
#   funtionality of the attention layer.
expected = softmax(matrix(1.76, rows=1, cols=1))
assert(toString(calculated) == toString(expected))

# ============== Simple Example ======================================
# For simplicity and since systemDS has no embedding at the time of the
#   development of this test, we simply used numbers from
#   https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767.
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers from the link above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" x 4(d_models)
input_embeddings = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)
print("Input embeddings:\n" + toString(input_embeddings))
# initial weight matrices with 4 (d_models) x 3(d_k /d_v)
q_weights = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
k_weights = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
v_weights = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)
print("q_weights:\n" + toString(q_weights))
print("k_weights:\n" + toString(k_weights))
print("v_weights:\n" + toString(v_weights))

# 3. attention layer
query = input_embeddings %*% q_weights
key = input_embeddings %*% k_weights
value = input_embeddings %*% v_weights

attention = attention::forward(query, key, value)
print("attention: \n" + toString(attention))
# Results in scores:
#   0.136 0.432 0.432
#   0.001 0.909 0.090
#   0.007 0.755 0.238

# Attention vectors:
#   1.864 6.319 1.704
#   1.999 7.814 0.273
#   1.993 7.480 0.736

# Which is roughly the same result as in the article.

# ================ Practical example =================
# Similar to the example from https://keras.io/api/layers/attention_layers/attention/
# 1. input eg text data
input = "SystemsDS is cool"

# 2. embedding layer
# Here we simply use the numbers from the link as in the example above, normaly they are generated by embedding
#    -> 3 tokens in "SystemsDS is cool" x 4 (d_models)
token_embedding = matrix("1 0 1 0 0 2 0 2 1 1 1 1", rows=3, cols=4)
# initial weight matrices with 4 (d_models) x 3(d_k and d_v)
query_embeddings = matrix("1 0 1 1 0 0 0 0 1 0 1 1", rows=4, cols=3)
key_embeddings = matrix("0 0 1 1 1 0 0 1 0 1 1 0", rows=4, cols=3)
value_embeddings = matrix("0 2 0 0 3 0 1 0 3 1 1 0", rows=4, cols=3)

# 3. cnn layer
# Like this:
# query_seq_encoding = cnn_layer(query_embeddings)
# query_seq_encoding = cnn_layer(key_embeddings)
# query_seq_encoding = cnn_layer(value_embeddings)

# In system DS:
# input = 4x3
# filter = 1x3
# pad = 0
# stride = 1
# numInput = 4
# numChannesl = 1
# inputWidth = 3
# numFilters = 1
# filterSize = 3
filter = matrix("1 1 1", rows=1, cols=3) # TODO fine this way?
query_seq_encoding = conv1d::forward(query_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)
key_seq_encoding = conv1d::forward(key_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)
value_seq_encoding = conv1d::forward(value_embeddings, filter, 0, 1, 4, 1, 3, 1, 3)

# 4. attention layer
# query_value_attention_seq = tf.keras.layers.Attention()(
# [query_seq_encoding, value_seq_encoding])
attention_seq = attention::forward(query_seq_encoding, key_seq_encoding, value_seq_encoding)


# 5. Global Average Pooling
# query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
# query_seq_encoding)
# query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
# query_value_attention_seq)

# In system DS:
# systemds/src/test/scripts/applications/nn/grad_check.dml lines 1660 ff sind nice als bsp
# ------- Experimenting -----
# out = conv2d(input, filter, padding=[0,pad], stride=[1, stride],
# input_shape=[numInput,numChannels,1,inputWidth], filter_shape=[numFilters,numChannels,1,filterSize])
# }
print("stuff 1:")
print(toString(query_seq_encoding))
print("stuff 2:")
print(toString(attention_seq))
# ---------------------------
# X = 4x1
# C = 1
# Hin = 4
# Win = 1
# Hf = 1
# Wf = 1
# strideh = 1
# stridew = 1
# padh = 0
# padw = 1
# exmaple with dimension != 1
  # [testing_, Hout, Wout] = avg_pool2d::forward(query_embeddings, 3, 1, 1, 1, 1, 1, 1, 0, 0)
  # print(toString(testing_))
# TODO bug, no change of values - is this fine?
[query_encoding, Hout, Wout] = avg_pool2d::forward(query_seq_encoding, 1, 1, 1, 1, 1, 1, 1, 0, 0)
[attention, Hout, Wout] = avg_pool2d::forward(attention_seq, 1, 1, 1, 1, 1, 1, 1, 0, 0)

print("Results:")
print(toString(query_encoding))
print(toString(attention))


# 6. concatenate query and encodings to produe DNN input layer.
# input_layer = tf.keras.layers.Concatenate()(
# [query_encoding, query_value_attention])


