#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("nn/layers/softmax.dml") as softmax


forward = function(matrix[double] query, matrix[double] key, matrix[double] value)
    return (matrix[double] attention) {
    /*
     * Computes the forward pass for the attention layer.
     *
     * Inputs:
     * - query: Input query(s) of shape (N,M).
     * - key: Key(s) for value(s) of shape (N,M).
     * - value: Value(s) for key(s) of shape (N,L).
     *
     * Outputs:
     * - attention: Attention on value(s) for given query(s), of shape (N,L).
     */
    M = ncol(key)
    key_norm = key / M^0.5
    scores = query %*% t(key_norm)
    #column wise softmax
    probs = t(softmax::forward(t(scores)))
    attention = probs %*% value
}

backward = function(matrix[double] dattention, matrix[double] query, matrix[double] key, matrix[double] value)
    return (matrix[double] dquery, matrix[double] dkey, matrix[double] dvalue)
{
    /*
     * Computes the backward pass for the attention layer.
     *
     * Inputs:
     * - dattention: Gradient wrt `attention` of shape (N,L).
     * - query: Query input of shape (N,M).
     * - key: Keys for values of shape (N,M).
     * - value: Values for given key of shape (N,L).
     * 
     * Outputs:
     * - dquery: Gradient wrt `query`, of shape (N,M).
     * - dkey: Gradient wrt `key`, of shape (N,M).
     * - dvalue: Gradient wrt `value` of shape (N,L).
     */

    N = nrow(query)
    M = ncol(query)
    L = ncol(value)

    norm = 1 / M^0.5

    key_norm = key * norm
    scores = query %*% t(key_norm)
    probs = t(softmax::forward(t(scores)))

    #TODO: Obviously inefficient computations, but let's check first if correct.
    dvalue = matrix(0, rows=N, cols=L)
    for (a in 1:N)
    {
        dvalue[a,] = t(probs[,a]) %*% dattention
    }

    dquery = matrix(0, rows=N, cols=M)
    for (i in 1:N)
    {
        for (j in 1:L)
        {
            for (k in 1:N)
            {
                for (a in 1:N)
                {
                    for (b in 1:M)
                    {
                        temp = matrix(0, rows=1, cols=N)
                        temp[,a] = key_norm[a,b]
                        dprobs = softmax::backward(temp, t(probs[,k]))
                        dquery[a,b] += value[k,j] * dprobs[,i] * dattention[i,j]
                    }
                }
            }
        }
    }

    dkey = matrix(0, rows=N, cols=M)
    for (i in 1:N)
    {
        for (j in 1:L)
        {
            for (k in 1:N)
            {
                for (a in 1:N)
                {
                    for (b in 1:M)
                    {
                        temp = matrix(0, rows=1, cols=N)
                        temp[,a] = query[a,b]
                        dprobs = softmax::backward(temp, t(probs[,k]))
                        dkey[a,b] += value[k,j] * dprobs[,i] * dattention[i,j]
                    }
                }
            }
        }
    }
    dkey = dkey * norm
}
