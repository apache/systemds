#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("nn/layers/softmax.dml") as softmax

# 1. Create "calculate_score" function, for the beginning without additional parameters -> calculate with dot product.

calculate_scores = function(matrix[double] query, matrix[double] key)
    return (matrix[double] scores) {
    /*
     * dim(query) = AxB
     * dim(key) = CxB
     * dim(scores) = AxC
     */
    scores = query %*% t(key)
}

apply_scores = function(matrix[double] scores, matrix[double] value, int d)
    return (matrix[double] attention) {
    /*
     * dim(scores) = MxM
     * dim(value) = MxL
     * dim(weights) =
     * dim(attention) = AxD
     */
    # put MxM matrix into M*Mx1 vector for matrix wise softmax
    # TODO: check if softmax is matrix wise or column wise or row wise
    x = matrix(scores / d ^ 0.5, cols=nrow(scores) * ncol(scores), rows=1)
    weights = matrix(softmax::forward(x), cols=ncol(scores), rows=nrow(scores))
    attention = weights %*% value
}

forward = function(matrix[double] query, matrix[double] key, matrix[double] value)
    return (matrix[double] attention) {
    /*
     * dim(query) = MxN
     * dim(key) = MxN
     * dim(value) = MxL
     * dim(scores) = MxM
     * dim(attention) = MxL
     */
    scores = calculate_scores(query, key)
    attention = apply_scores(scores, value, ncol(key))
}

backward = function(matrix[double] dattention, matrix[double] query, matrix[double] key, matrix[double] value)
    return (matrix[double] dquery, matrix[double] dkey, matrix[double]dvalue)
{
    /*
     * attention = weights * value
     * weights = softmax(v)
     * v = scores/ncol(value) ^ 0.5
     * scores = query * t(key)
     */
    scores = calculate_scores(query, key)

    M = nrow(query)
    N = ncol(query)
    L = ncol(value)

    norm = 1/N^0.5
    x = scores * norm
    y = matrix(softmax::forward(matrix(x, rows=1, cols=M*M)), rows=M, cols=M)
    dx = softmax::backward(y,x)

    dvalue = matrix(0, rows=M, cols=L)
    for (i in 1:M)
    {
        dvalue[,1:L] += t(y[i,])
    }

    #TODO: not sure how to add the derivative of softmax dx
    dquery = matrix(0, rows=M, cols=N)
    dquery_i = matrix(t(value) %*% key * norm, rows=M, cols=N)
    for (j in 1:M)
    {
        dquery[1:M] += dquery_i[j]
    }

    #TODO: not sure how to add the derivative of softmax dx
    dkey = matrix(0, rows=M, cols=N)
    v = matrix(0, rows=M, cols=1)
    for (i in 1:M)
    {
        v[i] = sum(value[i])
        q = matrix(0, rows=1, cols=N)
        for(j in 1:N)
        {
          dkey[i,j] = sum(query[,j] %*% v[i])
        }
    }
    dkey = dkey * norm
}
