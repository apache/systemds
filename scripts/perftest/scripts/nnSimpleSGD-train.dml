#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
# Imports
source("../../nn/layers/affine.dml") as affine
source("../../nn/layers/l2_loss.dml") as l2_loss
source("../../nn/layers/relu.dml") as relu
source("../../nn/optim/sgd.dml") as sgd

# read input data
X = read($X)
Y = read($Y)
fmt = ifdef($fmt, "csv")

N = nrow(X)
D = ncol(X)
t = ncol(Y)

# Create 2-layer network:
## affine1 -> relu1 -> affine2
M = ifdef($M, 64) # number of neurons  # todo parameterize this
[W1, b1] = affine::init(D, M, -1)
[W2, b2] = affine::init(M, t, -1)

# Initialize optimizer
lr = 0.05  # learning rate
mu = 0.9  # momentum
decay = 0.99  # learning rate decay constant

# Optimize
print("Starting optimization")
batch_size = ifdef($batch_size, 32)
epochs = ifdef($epochs, 5)
iters = N / batch_size
for (e in 1:epochs) {
  for(i in 1:iters) {
    # Get next batch
    X_batch = X[(i-1)*batch_size+1:i*batch_size,]
    y_batch = Y[(i-1)*batch_size+1:i*batch_size,]

    # Compute forward pass
    out1 = affine::forward(X_batch, W1, b1)
    outr1 = relu::forward(out1)
    out2 = affine::forward(outr1, W2, b2)

    # Compute loss
    loss = l2_loss::forward(out2, y_batch)
    print("L2 loss: " + loss)

    # Compute backward pass
    dout2 = l2_loss::backward(out2, y_batch)
    [doutr1, dW2, db2] = affine::backward(dout2, outr1, W2, b2)
    dout1 = relu::backward(doutr1, out1)
    [dX_batch, dW1, db1] = affine::backward(dout1, X_batch, W1, b1)

    # Optimize with vanilla SGD
    W1 = sgd::update(W1, dW1, lr)
    b1 = sgd::update(b1, db1, lr)
    W2 = sgd::update(W2, dW2, lr)
    b2 = sgd::update(b2, db2, lr)
  }
  # Decay learning rate
  lr = lr * decay
}
# save params
write(W1,""+$B+"/w1_simple_sgd",format=fmt)
write(W2,""+$B+"/w2_simple_sgd",format=fmt)
write(b1,""+$B+"/b1_simple_sgd",format=fmt)
write(b2,""+$B+"/b2_simple_sgd",format=fmt)