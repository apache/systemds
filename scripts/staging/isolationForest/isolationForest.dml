#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#==============================================================
# THIS SCRIPT IMPLEMENTS ANOMALY DETECTION VIA ISOLATION FOREST
# AS DESCRIBED IN 
# [Liu2008] Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008, December). 
#           Isolation forest. In 2008 eighth ieee international 
#           conference on data mining (pp. 413-422). IEEE.     
#==============================================================


# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read feature matrix X; note that X needs to be both recoded and dummy coded 
# Y 			String   ---		  Location to read label matrix Y; note that Y needs to be both recoded and dummy coded
# R   	  		String   " "	      Location to read the matrix R which for each feature in X contains the following information 
#										- R[,1]: column ids
#										- R[,2]: start indices 
#										- R[,3]: end indices
#									  If R is not provided by default all variables are assumed to be scale
# bins          Int 	 20			  Number of equiheight bins per scale feature to choose thresholds
# depth         Int 	 25			  Maximum depth of the learned tree
# num_leaf      Int      10           Number of samples when splitting stops and a leaf node is added
# num_samples   Int 	 3000		  Number of samples at which point we switch to in-memory subtree building
# impurity      String   "Gini"    	  Impurity measure: entropy or Gini (the default)
# M             String 	 ---	   	  Location to write matrix M containing the learned tree
# O     		String   " "          Location to write the training accuracy; by default is standard output
# S_map			String   " "		  Location to write the mappings from scale feature ids to global feature ids
# C_map			String   " "		  Location to write the mappings from categorical feature ids to global feature ids
# fmt     	    String   "text"       The output format of the model (matrix M), such as "text" or "csv"
# ---------------------------------------------------------------------------------------------
# OUTPUT: 
# M
# -------------------------------------------------------------------------------------------
outlierByIsolationForest = function(Matrix[Double] X, 
  Int n_trees, Int subsampling_size) 
  return(Matrix[Double] M)
{
  M = matrix(0, cols=1, rows=1)
}

outlierByIsolationForestApply = function(Matrix[Double] X, Matrix[Double] M)
  return(Matrix[Double] Y)
{
  Y = matrix(0, cols=1, rows=1)
}

# This function implements isolation forest for numerical input features as 
# described in [Liu2008]
#
# .. code-block::
#
#   For example, give a feature matrix with features [a,b,c,d]
#   and the following trees, M would look as follows:
#
#   Level              Tree 1                  Tree 2        Node Depth
#   -------------------------------------------------------------------                    
#   (L1)               |d<=5|                  |b<=6|           0
#                     /     \                 /      \  
#   (L2)           P1:2    |a<=7|          P1:2      P2:5       1 
#                          /   \
#   (L3)                 P2:2 P3:1                              2 
#
#   --> M :=
#   [[ 4, 5,  0,  2,  1,  7, -1, -1, -1, -1,  0,  2,  0],  (Tree 1)
#    [ 1, 2,  6,  0,  2,  0,  5, -1, -1, -1, -1, -1, -1]]  (Tree 2)
#    | (L1) | |     (L2)      | |          (L3)         | 
#
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME              TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X                 Matrix[Double]                Numerical feature matrix
# n_trees   	      Int               		        Number of iTrees to build
# subsampling_size  Int                           Size of the subsample to build iTrees with 
# bootstrap         Boolean                       Indicator if subsampling should be done with 
#                                                 replacement or not 
# seed              Int              -1           Seed for calls to `sample` and `rand`.
#                                                 -1 corresponds to a random seed
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# M    Matrix containing the learned iForest in linearized form
# ---------------------------------------------------------------------------------------------
m_iForest = function(Matrix[Double] X, Int n_trees, Int subsampling_size, Int seed = -1) 
  return(Matrix[Double] M)
{
  # check assumptions
  s_warning_assert(n_trees > 0, "iForest: Requirement n_trees > 1 not satisfied! ntrees: "+toString(n_trees))
  s_warning_assert(subsampling_size > 1 & subsampling_size <= nrow(X), "iForest: Requirement 0 < subsampling_size <= nrow(X) not satisfied! subsampling_size: "+toString(subsampling_size)+"; nrow(X): "+toString(nrow(X)))

  height_limit = ceil(log(subsampling_size, 2))  
  tree_size = 2*(2^(height_limit+1)-1)

  # initialize the model
  M = matrix(-1, cols=tree_size, rows=n_trees)

  for (i_iTree in 1:n_trees) {
    # subsample rows
    seed = ifelse(seed == -1, -1, i_iTree*seed)
    X_subsample = s_sampleRows(X, subsampling_size, seed)

    # Build iTree
    seed = ifelse(seed == -1, -1, seed+42)
    M_tree = m_iTree(X, height_limit, seed)

    # Add iTree to the model
    M[i_iTree, 1:ncol(M_tree)] = M_tree
  }
}

# This function implements isolation trees for numerical input features as 
# described in [Liu2008] 
# TODO describe the structure
#
# .. code-block::
#
#   For example, give a feature matrix with features [a,b,c,d]
#   and the following tree, M would look as follows:
#   Level              Tree                Node Depth
#   ------------------------------------------------- 
#   (L1)               |d<5|                   0
#                     /     \
#   (L2)           P1:2    |a<7|               1 
#                          /   \
#   (L3)                 P2:2 P3:1             2 
#
#   --> M :=
#   [[4, 5, 0, 2, 1, 7, -1, -1, -1, -1, 0, 2, 0, 1]]
#    |(L1)| |  (L2)   | |         (L3)           |
#
#
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME            TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X               Matrix[Double]                Numerical feature matrix
# max_depth   	  Int               		        Maximum depth of the learned tree where depth is the 
#                                               maximum number of edges from root to a leaf note
# seed            Int              -1           Seed for calls to `sample` and `rand`.
#                                               -1 corresponds to a random seed
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# M    Matrix M containing the learned tree in linearized form
# ---------------------------------------------------------------------------------------------
m_iTree = function(Matrix[Double] X, Int max_depth, Int seed = -1)
  return(Matrix[Double] M) 
{
  # check assumptions
  s_warning_assert(max_depth > 0 & max_depth <= 32, "iTree: Requirement 0 < max_depth < 32 not satisfied! max_depth: " + max_depth)
  s_warning_assert(nrow(X) > 0, "iTree: Feature matrix X has no less than 2 rows!")


  # Initialize M to largest possible matrix given max_depth 
  # Note that each node takes exactly 2 indices in M and the root node has depth 0
  M = matrix(-1, rows=1, cols=2*(2^(max_depth+1)-1))
  
  # Queue for implementing recursion in the original algorithm.
  # Each entry in the queue corresponds to a node that in the tree to be added to the model 
  # M and, in case of internal nodes, split further.
  # Nodes in this queue are represented by an ID (first index) and the data corrseponding to the node (second index)
  node_queue = list(list(1, X));
  # variable tracking the maximum ID of in the tree
  max_id = 1;
  while (length(node_queue) > 0) {
    # pop next node from queue for splitting
    [node_queue, queue_entry] = remove(node_queue, 1);
    node = as.list(queue_entry);
    node_id = as.scalar(node[1]);
    X_node = as.matrix(node[2]);

    max_id = max(max_id, node_id)

    is_external_leaf = s_isExternalINode(X_node, node_id, max_depth)
    if (is_external_leaf) {
      # External Node: Add node to model
      M = s_addExternalINode(X_node, node_id, M)
    }
    else {
      # Internal Node: Draw split criterion, add node to model and queue child nodes
      seed = ifelse(seed == -1, -1, node_id*seed)
      [split_feature, split_value] = s_drawSplitPoint(X_node, seed)
      M = s_addInternalINode(node_id, split_feature, split_value, M)
      [left_id, X_left, right_id, X_right] = s_splitINode(X_node, node_id, split_feature, split_value)
      
      node_queue = append(node_queue, list(left_id, X_left))
      node_queue = append(node_queue, list(right_id, X_right))
    }    
  }

  # Prune the model to the actual tree depth
  tree_depth = floor(log(max_id, 2))
  M = M[1, 1:2*(2^(tree_depth+1) - 1)];
}


# Randomly draws a split point i.e. a feature and corresponding value to split a node by.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             Matrix[Double]                Numerical feature matrix
# seed          Int              -1           Seed for calls to `sample` and `rand`  
#                                             -1 corresponds to a random seed
# 
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# split_feature   Index of the feature used for splitting the node
# split_value     Feature value used for splitting the node 
# ---------------------------------------------------------------------------------------------
s_drawSplitPoint = function(Matrix[Double] X, Int seed = -1) 
  return(Int split_feature, Double split_value)
{
  # find random feature and a value between the min and max values of that feature to split the node by
  split_feature = as.integer(as.scalar(sample(ncol(X), 1, FALSE, seed)))
  split_value = as.scalar(rand(
    rows=1, cols=1,
    min=min(X[, split_feature]),
    max=max(X[, split_feature]),
    seed=seed
  ))
}

# Adds a external (leaf) node to the linearized iTree model `M`. In the linerized form, 
# each node is assigned two neighboring indices. For external nodes the value at the first 
# index in M is always set to 0 while the value at the second index is set to the number of
# rows in the feature matrix corresponding to the node.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X_node        Matrix[Double]                Numerical feature matrix corresponding to the node
# node_id       Int                           ID of the node
# M             Matrix[Double]                Linerized model to add the node to
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# M   The updated model
# ---------------------------------------------------------------------------------------------
s_addExternalINode = function(Matrix[Double] X_node, Int node_id, Matrix[Double] M) 
  return(Matrix[Double] M)
{
  s_warning_assert(node_id > 0, "s_addExternalINode: node_id > 0")
  
  node_start_index = 2*node_id-1
  M[, node_start_index] = 0
  M[, node_start_index + 1] = nrow(X_node)
}

# Adds a internal node to the linearized iTree model `M`. In the linerized form, 
# each node is assigned two neighboring indices. For internal nodes the value at the first 
# index in M is set to index of the feature to split by while the value at the second index 
# is set to the value to split the node by.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME           TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# node_id        Int                           ID of the node
# split_feature  Int                           Index of the feature to split the node by
# split_value    Int                           Value to split the node by
# M              Matrix[Double]                Linerized model to add the node to
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# M   The updated model
# ---------------------------------------------------------------------------------------------
s_addInternalINode = function(Int node_id, Int split_feature, Double split_value, Matrix[Double] M)
  return(Matrix[Double] M)
{
  s_warning_assert(node_id > 0, "s_addInternalINode: node_id > 0")
  s_warning_assert(split_feature > 0, "s_addInternalINode: split_feature > 0")

  node_start_index = 2*node_id-1
  M[, node_start_index] = split_feature
  M[, node_start_index + 1] = split_value
}

# This function determines if a iTree node is an external node based on it's node_id and the data corresponding to the node   
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME            TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X_node        Matrix[Double]                  Numerical feature matrix corresponding to the node
# node_id   	  Int               		          ID belonging to the node
# max_depth   	  Int               		        Maximum depth of the learned tree where depth is the 
#                                               maximum number of edges from root to a leaf note
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# isExternalNote   true if the node is an external (leaf) node, false otherwise.
#                  This is the case when a max depth is reached or the number of rows 
#                  in the feature matrix corresponding to the node <= 1
# ---------------------------------------------------------------------------------------------
s_isExternalINode = function(Matrix[Double] X_node, Int node_id, Int max_depth) 
  return(Boolean isExternalNode)
{
  s_warning_assert(max_depth > 0, "s_isExternalINode: max_depth > 0")
  s_warning_assert(node_id > 0, "s_isExternalINode: node_id > 0")

  node_depth = floor(log(node_id, 2))
  isExternalNode = node_depth >= max_depth | nrow(X_node) <= 1
}


# This function splits a node based on a given feature and value and returns the sub-matrices 
# and IDs corresponding to the nodes resulting from the split.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME            TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X_node        Matrix[Double]                  Numerical feature matrix corresponding
# node_id       Int                             ID of the node to split   
# split_feature Int                             Index of the feature to split the input matrix by 
# split_value   Int                             Value of the feature to split the input matrix by 
#
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# left_id    ID of the resulting left node
# X_left     Matrix corresponding to the left node resulting from the split with rows where 
#            value for feature `split_feature` <= value `split_value`
# right_id   ID of the resulting right node
# X_right    Matrix corresponding to the left node resulting from the split with rows where 
#            value for feature `split_feature` > value `split_value`
# ---------------------------------------------------------------------------------------------
s_splitINode = function(Matrix[Double] X_node, Int node_id, Int split_feature, Double split_value) 
  return(Int left_id, Matrix[Double] X_left, Int right_id, Matrix[Double] X_right)
{
  s_warning_assert(nrow(X_node) > 0, "s_splitINode: nrow(X_node) > 0")
  s_warning_assert(node_id > 0, "s_splitINode: nrow(X_node) > 0")
  s_warning_assert(split_feature > 0, "s_splitINode: split_feature > 0")

  left_rows_mask = X_node[, split_feature] <= split_value 

  # In the lineraized form of the iTree model, nodes need to be ordered by depth
  # Since iTrees are binary trees we can use 2*node_id/2*node_id+1 for left/right child ids 
  # to insure that IDs are chosen accordingly.
  left_id = 2 * node_id
  X_left = removeEmpty(target=X_node, margin="rows", select=left_rows_mask, empty.return=FALSE)

  right_id = 2 * node_id + 1
  X_right = removeEmpty(target=X_node, margin="rows", select=!left_rows_mask, empty.return=FALSE)
}

# Randomly samples `size` rows from a matrix X
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME            TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             Matrix[Double]                  Matrix to sample rows from
# sample_size   Int                             Number of rows to sample
# seed          Int              -1             Seed for calls to `sample`
#                                               -1 corresponds to a random seed
#
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# X_sampled    Sampled rows from X
# ---------------------------------------------------------------------------------------------
s_sampleRows = function(Matrix[Double] X, Int size, Int seed = -1)
  return(Matrix[Double] X_extracted)
{
  s_warning_assert(size > 0 & nrow(X) >= size, "s_sampleRows: size > 0 & nrow(X) >= size")
  random_vector = rand(rows=nrow(X), cols=1, seed=seed)
  X_rand = cbind(X, random_vector)

  # order by random vector and return `size` nr of rows`
  X_rand = order(target=X_rand, by=ncol(X_rand))
  X_extracted = X_rand[1:size, 1:ncol(X)]
}

# Calculates the PathLength as defined in [Liu2008] based on a sample x
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# M             Matrix[Double]                The linearized iTree model
# x             Matrix[Double]                The sample to calculate the PathLength
#
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# PathLength  The PathLength for the sample
# ---------------------------------------------------------------------------------------------
m_PathLength = function(Matrix[Double] M, Matrix[Double] x)
  return(Double PathLength)
{
  [nrEdgesTraversed, externalNodeSize] = s_traverseITree(M, x)
  
  if (externalNodeSize <= 1) {
    PathLength = nrEdgesTraversed   
  }
  else {
    PathLength = nrEdgesTraversed + s_cn(externalNodeSize)
  }
}


# Traverses an iTree based on a sample x
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# M             Matrix[Double]                The linearized iTree model to traverse
# x             Matrix[Double]                The sample to traverse the iTree with
#
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# nrEdgesTraversed         The number of edges traversed until an external node was reached
# externalNodeSize   The size of of the external node assigned to during training
# ---------------------------------------------------------------------------------------------
s_traverseITree = function(Matrix[Double] M, Matrix[Double] x)
  return(Int nrEdgesTraversed, Int externalNodeSize)
{
  s_warning_assert(nrow(x) == 1, "s_traverseITree: nrow(x) == 1")

  nrEdgesTraversed = 0
  is_external_node = FALSE
  node_id = 1
  while (!is_external_node)
  {
    node_start_idx = (node_id*2) - 1
    split_feature = as.integer(as.scalar(M[1,node_start_idx]))
    node_value = as.scalar(M[1,node_start_idx + 1])

    if (split_feature > 0) {
      # internal node - node_value = split_value
      nrEdgesTraversed = nrEdgesTraversed + 1
      x_val = as.scalar(x[1, split_feature])
      if (x_val <= node_value) {
        # go down left
        node_id = (node_id * 2)
      }
      else {
        # go down right
        node_id = (node_id * 2) + 1
      }
    }
    else if (split_feature == 0) {
      # External node - node_value = node size
      externalNodeSize = as.integer(node_value)
      is_external_node = TRUE
    }
    else {
      s_warning_assert(FALSE, "iTree is not valid!")
    }
  } 
}


# This function gives the average path length of unsuccessful search in BST `c(n)`
# for `n` nodes as given in [Liu2008]. This function is used to normalize the path length
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# n             Int                           Number of samples that corresponding to an external
#                                             node for which c(n) should be calculated
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# cn   Value for c(n)
# ---------------------------------------------------------------------------------------------
s_cn = function(Int n)
  return(Double cn)
{
  s_warning_assert(n > 1, "s_cn: n > 1")
  
  # Calculate H(n-1)
  # The approximation of the Harmonic Number H using `log(n) + eulergamma` has a higher error
  # for low n. We hence calculate it directly for the first 1000 values
  # TODO: Discuss a good value for n --> use e.g. HarmonicNumber(1000) - (ln(1000) + 0.5772156649) in WA
  if (n < 1000) {
    H_nminus1 = 0
    for (i in 1:n-1) 
      H_nminus1 = H_nminus1 + 1/i;
  }
  else{
    # Euler–Mascheroni's constant
    eulergamma = 0.57721566490153
    # Approximation harmonic number H(n - 1)
    H_nminus1 = log(n-1) + eulergamma
  }

  cn = 2*H_nminus1 - 2*(n-1)/n
}

m_score = function(Matrix[Double] M, Matrix[Double] x, Int n)
  return(Double score)
{
  score = 3
}

# Function that gives a warning if a assertion is violated. This is used instead of `assert` and
# `stop` since these function can not be used in parfor .
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE             DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# n             Int                           Number of samples that corresponding to an external
#                                             node for which c(n) should be calculated
# ---------------------------------------------------------------------------------------------
# OUTPUT PARAMETERS: 
# ---------------------------------------------------------------------------------------------
# cn   Value for c(n)
# ---------------------------------------------------------------------------------------------
s_warning_assert = function(Boolean assertion, String warning) 
{
  if (!assertion)
    print("WARNING! "+warning)
}