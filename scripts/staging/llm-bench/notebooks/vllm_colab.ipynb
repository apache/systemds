{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SYSTEMDS-BENCH-GPT: vLLM Benchmarking\n",
        "\n",
        "This notebook runs all vLLM benchmarks on Google Colab's GPU.\n",
        "\n",
        "**Steps:**\n",
        "1. Check GPU and install dependencies\n",
        "2. Clone/update repository\n",
        "3. Start vLLM server\n",
        "4. Run all 4 workloads\n",
        "5. Download results\n",
        "\n",
        "**Requirements:** Enable GPU runtime (Runtime → Change runtime type → T4 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install vllm torch transformers accelerate -q\n",
        "!pip install pyyaml numpy tqdm datasets requests psutil rouge-score -q\n",
        "print(\"\\n✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('/content/systemds-bench-gpt'):\n",
        "    print(\"Repository exists, pulling latest...\")\n",
        "    %cd /content/systemds-bench-gpt\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/kubraaksux/systemds-bench-gpt.git\n",
        "    %cd /content/systemds-bench-gpt\n",
        "\n",
        "print(\"\\n✓ Repository ready\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start vLLM Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start vLLM server\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# ========== MODEL SELECTION ==========\n",
        "\n",
        "# Option 1: phi-2 (2.7B) - Fast, good for testing\n",
        "MODEL = \"microsoft/phi-2\"\n",
        "\n",
        "# Option 2: Llama-2-7B - Better accuracy, fits in T4 (requires HF login)\n",
        "# MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Option 3: TinyLlama (1.1B) - Fastest, lowest accuracy\n",
        "# MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# =====================================\n",
        "\n",
        "# kill any existing server\n",
        "!pkill -f \"vllm.entrypoints\" 2>/dev/null || True\n",
        "time.sleep(2)\n",
        "\n",
        "print(f\"Starting vLLM server with model: {MODEL}\")\n",
        "print(\"This takes 4-6 minutes (download + load + compile CUDA graphs)...\")\n",
        "print()\n",
        "\n",
        "# start server in background\n",
        "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "    --model {MODEL} \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8000 \\\n",
        "    --dtype float16 > vllm_server.log 2>&1 &\n",
        "\n",
        "# wait for server to start\n",
        "print(\"Waiting for model to load...\")\n",
        "for i in range(72):  \n",
        "    time.sleep(5)\n",
        "    elapsed = (i+1)*5\n",
        "    mins = elapsed // 60\n",
        "    secs = elapsed % 60\n",
        "    print(f\"  {mins}m {secs}s...\", end=\"\")\n",
        "    try:\n",
        "        resp = requests.get(\"http://localhost:8000/v1/models\", timeout=5)\n",
        "        if resp.status_code == 200:\n",
        "            print(\"\\n\\n\" + \"=\"*50)\n",
        "            print(\"✓ vLLM SERVER IS READY!\")\n",
        "            print(\"=\"*50)\n",
        "            print(resp.json())\n",
        "            break\n",
        "    except:\n",
        "        print(\" loading...\")\n",
        "else:\n",
        "    print(\"\\n\\nServer still loading. Check if process is running:\")\n",
        "    !ps aux | grep -E \"vllm|python\" | grep -v grep | head -5\n",
        "    print(\"\\nLatest logs:\")\n",
        "    !tail -30 vllm_server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# quick test to verify server works\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    resp = requests.get(\"http://localhost:8000/v1/models\", timeout=10)\n",
        "    print(\"✓ Server is running!\")\n",
        "    print(f\"  Models: {resp.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Server not ready: {e}\")\n",
        "    print(\"\\nRun the previous cell again or check logs:\")\n",
        "    !tail -30 vllm_server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run ALL Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run all 4 workloads\n",
        "import os\n",
        "os.chdir('/content/systemds-bench-gpt')\n",
        "\n",
        "workloads = [\n",
        "    (\"math\", \"results/vllm_math\"),\n",
        "    (\"reasoning\", \"results/vllm_reasoning\"),\n",
        "    (\"summarization\", \"results/vllm_summarization\"),\n",
        "    (\"json_extraction\", \"results/vllm_json\"),\n",
        "]\n",
        "\n",
        "for workload, output in workloads:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Running: {workload}\")\n",
        "    print(\"=\"*60)\n",
        "    !python runner.py \\\n",
        "        --backend vllm \\\n",
        "        --model {MODEL} \\\n",
        "        --workload workloads/{workload}/config.yaml \\\n",
        "        --out {output}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL BENCHMARKS COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display results summary\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"vLLM BENCHMARK RESULTS (microsoft/phi-2)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_dir = \"/content/systemds-bench-gpt/results\"\n",
        "for run_dir in sorted(os.listdir(results_dir)):\n",
        "    if run_dir.startswith(\"vllm_\"):\n",
        "        metrics_path = f\"{results_dir}/{run_dir}/metrics.json\"\n",
        "        if os.path.exists(metrics_path):\n",
        "            with open(metrics_path) as f:\n",
        "                m = json.load(f)\n",
        "            workload = run_dir.replace(\"vllm_\", \"\")\n",
        "            acc = m.get('accuracy_mean', 0) * 100\n",
        "            acc_count = m.get('accuracy_count', 'N/A')\n",
        "            lat = m.get('latency_ms_p50', 0)\n",
        "            thr = m.get('throughput_req_per_s', 0)\n",
        "            \n",
        "            print(f\"\\n{workload.upper()}:\")\n",
        "            print(f\"  Accuracy:   {acc:.0f}% ({acc_count})\")\n",
        "            print(f\"  Latency:    {lat:.0f}ms (p50)\")\n",
        "            print(f\"  Throughput: {thr:.3f} req/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# zip and download all vLLM results\n",
        "import os\n",
        "os.chdir('/content/systemds-bench-gpt')\n",
        "\n",
        "!zip -r vllm_results_final.zip results/vllm_*\n",
        "\n",
        "from google.colab import files\n",
        "files.download('vllm_results_final.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DOWNLOAD COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext steps in your local IDE:\")\n",
        "print(\"1. unzip ~/Downloads/vllm_results_final.zip -d results/\")\n",
        "print(\"2. python scripts/report.py --out benchmark_report.html\")\n",
        "print(\"3. open benchmark_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stop the vLLM server to free GPU memory\n",
        "!pkill -f \"vllm.entrypoints\" || True\n",
        "print(\"✓ vLLM server stopped\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
