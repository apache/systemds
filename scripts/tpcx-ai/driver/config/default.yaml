#
# Copyright (C) 2021 Transaction Processing Performance Council (TPC) and/or its contributors.
# This file is part of a software package distributed by the TPC
# The contents of this file have been developed by the TPC, and/or have been licensed to the TPC under one or more contributor
# license agreements.
# This file is subject to the terms and conditions outlined in the End-User
# License Agreement (EULA) which can be found in this distribution (EULA.txt) and is available at the following URL:
# http://www.tpc.org/TPC_Documents_Current_Versions/txt/EULA.txt
# Unless required by applicable law or agreed to in writing, this software is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, and the user bears the entire risk as to quality
# and performance as well as the entire cost of service or repair in case of defect. See the EULA for more details.
#


#
# Copyright 2019 Intel Corporation.
# This software and the related documents are Intel copyrighted materials, and your use of them 
# is governed by the express license under which they were provided to you ("License"). Unless the 
# License provides otherwise, you may not use, modify, copy, publish, distribute, disclose or 
# transmit this software or the related documents without Intel's prior written permission.
# 
# This software and the related documents are provided as is, with no express or implied warranties, 
# other than those that are expressly stated in the License.
# 
#


# DEFAULT Configuration for the TPCx-AI Driver
# convenience configurations
# examples for different datastores

# Local filesystem on Windows, Linux, or MacOS

local_fs: &LOCAL_FS
  name: "local_fs"
  create: "tools/python/create.sh $destination"
  load: "tools/python/load.sh $destination $source"
  copy: "cp -f $source $destination"
  delete: "rm -rf $destination"
  delete_parallel: "pssh -t 0 -P -h nodes rm -rf $destination"
  download: "cp $source $destination"

# HDFS = Hadoop Distributed Filesystem
hdfs: &HDFS
  name: "hdfs"
  create: "tools/spark/create_hdfs.sh $destination"
  load: "tools/spark/load_hdfs.sh $destination $source"
  copy: "hdfs dfs -cp -f $source $destination"
  delete: "hdfs dfs -rm -r -f -skipTrash $destination"
  download: 'hdfs dfs -cat $source/* | awk ''BEGIN{f=""}{if($0!=f){print $0}if(NR==1){f=$0}}'' > $destination/predictions.csv'

workload:
  # global definitions
  engine_executable: &ENGINE "lib/python-venv/bin/python"
  engine_executable_ks: &ENGINE_KS "lib/python-venv-ks/bin/python"
  
  training_template: &TRAINING_TEMPLATE "$engine $name --stage training --workdir $output $input/$file"
  serving_template: &SERVING_TEMPLATE "$engine $name --stage serving  --workdir $model --output $output/$phase $input/$file"
  serving_throughput_template: &SERVING_THROUGHPUT_TEMPLATE "$engine $name --stage serving --workdir $model --output $output/$stream $input/$file"
  training_data_url: &TRAINING_DATA_URL "output/data/training"
  serving_data_url: &SERVING_DATA_URL "output/data/serving"
  scoring_data_url: &SCORING_DATA_URL "output/data/scoring"
  datagen_datastore: *LOCAL_FS
  include_datagen_in_tload: False
  # general/ benchmark-wide configuration parameters
  pdgf_node_parallel: False
  pdgf_home: "lib/pdgf"
  raw_data_url: "output/raw_data"
  temp_dir: '/tmp/tpcxai'
  usecases:
    1:
      # general
      name: "-m workload.UseCase01"
      # engines
      training_engine: *ENGINE
      serving_engine: *ENGINE
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --num_clusters 4 --workdir $output $input/order.csv $input/lineitem.csv $input/order_returns.csv"
      serving_template: "$engine $name --stage serving --workdir $model --output $output/$phase $input/order.csv $input/lineitem.csv $input/order_returns.csv"
      serving_throughput_template: "$engine $name --stage serving --workdir $model --output $output/$stream $input/order.csv $input/lineitem.csv $input/order_returns.csv"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc01"
      output_url: "output/output/uc01"
      scoring_output_url: "output/scoring/uc01"
    2:
      # general
      name: "-m workload.UseCase02"
      # engines
      training_engine: *ENGINE_KS
      serving_engine: *ENGINE_KS
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --epochs 25 --batch 32 --workdir $output $input/$file"
      serving_template: "$engine $name --stage serving --batch 32 --workdir $model --output $output/$phase $input/$file"
      serving_throughput_template: "$engine $name --stage serving --batch 32 --workdir $model --output $output/$stream $input/$file"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc02"
      output_url: "output/output/uc02"
      scoring_output_url: "output/scoring/uc02"
    3:
      # general
      name: "-m workload.UseCase03"
      # engines
      training_engine: *ENGINE
      serving_engine: *ENGINE
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --workdir $output $input/order.csv $input/lineitem.csv $input/product.csv"
      serving_template: "$engine $name --stage serving  --workdir $model --output $output/$phase $input/store_dept.csv"
      serving_throughput_template: "$engine $name --stage serving  --workdir $model --output $output/$stream $input/store_dept.csv"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc03"
      output_url: "output/output/uc03"
      scoring_output_url: "output/scoring/uc03"
    4:
      # general
      name: "-m workload.UseCase04"
      # engines
      training_engine: *ENGINE
      serving_engine: *ENGINE
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: *TRAINING_TEMPLATE
      serving_template: *SERVING_TEMPLATE
      serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc04"
      output_url: "output/output/uc04"
      scoring_output_url: "output/scoring/uc04"
    5:
      # general
      name: "-m workload.UseCase05"
      # engines
      training_engine: *ENGINE_KS
      serving_engine: *ENGINE_KS
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --epochs 15 --batch 512 --workdir $output $input/$file"
      serving_template: "$engine $name --stage serving --batch 512 --workdir $model --output $output/$phase $input/$file"
      serving_throughput_template: "$engine $name --stage serving --batch 512 --workdir $model --output $output/$stream $input/$file"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc05"
      output_url: "output/output/uc05"
      scoring_output_url: "output/scoring/uc05"
    6:
      # general
      name: "-m workload.UseCase06"
      # engines
      training_engine: *ENGINE_KS
      serving_engine: *ENGINE_KS
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: *TRAINING_TEMPLATE
      serving_template: *SERVING_TEMPLATE
      serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc06"
      output_url: "output/output/uc06"
      scoring_output_url: "output/scoring/uc06"
    7:
      # general
      name: "-m workload.UseCase07"
      # engines
      training_engine: *ENGINE_KS
      serving_engine: *ENGINE_KS
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: *TRAINING_TEMPLATE
      serving_template: *SERVING_TEMPLATE
      serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc07"
      output_url: "output/output/uc07"
      scoring_output_url: "output/scoring/uc07"
    8:
      # general
      name: "-m workload.UseCase08"
      # engines
      training_engine: *ENGINE
      serving_engine: *ENGINE
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --num-rounds 100 --workdir $output $input/order.csv $input/lineitem.csv $input/product.csv"
      serving_template: "$engine $name --stage serving  --workdir $model --output $output/$phase $input/order.csv $input/lineitem.csv $input/product.csv"
      serving_throughput_template: "$engine $name --stage serving  --workdir $model --output $output/$stream $input/order.csv $input/lineitem.csv $input/product.csv"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc08"
      output_url: "output/output/uc08"
      scoring_output_url: "output/scoring/uc08"
    9:
      # general
      name: "-m workload.UseCase09"
      # engines
      training_engine: *ENGINE_KS
      serving_engine: *ENGINE_KS
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --epochs_embedding=15 --batch=64 --workdir $output $input/CUSTOMER_IMAGES"
      serving_template: "$engine $name --stage serving --batch=64  --workdir $model --output $output/$phase $input/CUSTOMER_IMAGES"
      serving_throughput_template: "$engine $name --stage serving --batch=64  --workdir $model --output $output/$stream $input/CUSTOMER_IMAGES"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc09"
      output_url: "output/output/uc09"
      scoring_output_url: "output/scoring/uc09"
    10:
      # general
      name: "-m workload.UseCase10"
      # engines
      training_engine: *ENGINE
      serving_engine: *ENGINE
      # data stores
      training_datastore: *LOCAL_FS   # for storing the training data
      model_datastore: *LOCAL_FS      # for storing the trained models
      serving_datastore: *LOCAL_FS    # for storing the serving data
      output_datastore: *LOCAL_FS     # for storing the final output
      # templates
      datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
      training_template: "$engine $name --stage training --workdir $output $input/financial_account.csv $input/financial_transactions.csv"
      serving_template: "$engine $name --stage serving --workdir $model --output $output/$phase $input/financial_account.csv $input/financial_transactions.csv"
      serving_throughput_template: "$engine $name --stage serving --workdir $model --output $output/$stream $input/financial_account.csv $input/financial_transactions.csv"
      # URLs
      training_data_url: *TRAINING_DATA_URL
      serving_data_url: *SERVING_DATA_URL
      scoring_data_url: *SCORING_DATA_URL
      model_url: "output/model/uc10"
      output_url: "output/output/uc10"
      scoring_output_url: "output/scoring/uc10"
