#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("scripts/nn/layers/affine.dml") as affine
source("scripts/nn/layers/conv2d_builtin.dml") as conv2d
source("scripts/nn/layers/conv2d_transpose.dml") as conv2d_transpose
source("scripts/nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("scripts/nn/layers/dropout.dml") as dropout
source("scripts/nn/layers/l2_reg.dml") as l2_reg
source("scripts/nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("scripts/nn/layers/relu.dml") as relu
source("scripts/nn/layers/softmax.dml") as softmax
source("scripts/nn/optim/sgd_momentum.dml") as sgd_momentum
source("scripts/nn/layers/dropout.dml") as dropout

/*
 * Trains a convolutional net using the "Unet" architecture.
 *
 * The input matrix, X, has N examples, each represented as a 3D
 * volume unrolled into a single vector.  The targets, Y, have K
 * classes, and are one-hot encoded.
 *
 * Inputs:
 *  - X: Input data matrix, of shape (N, C*Hin*Win)
 *  - y: Target matrix, of shape (N, K)
 *  - X_val: Input validation data matrix, of shape (N, C*Hin*Win)
 *  - y_val: Target validation matrix, of shape (N, K)
 *  - C: Number of input channels (dimensionality of input depth)
 *  - Hin: Input height
 *  - Win: Input width
 *  - epochs: Total number of full training loops over the full data set
 *  - batch_size: Batch size
 *  - learning_rate: The learning rate for the SGD
 *
 * Outputs:
 *  - model_trained: List containing
 *       - W1: 1st layer weights (parameters) matrix, of shape (F1, C*Hf*Wf)
 *       - b1: 1st layer biases vector, of shape (F1, 1)
 *       - W2: 2nd layer weights (parameters) matrix, of shape (F2, F1*Hf*Wf)
 *       - b2: 2nd layer biases vector, of shape (F2, 1)
 *       - W3: 3rd layer weights (parameters) matrix, of shape (F2*(Hin/4)*(Win/4), N3)
 *       - b3: 3rd layer biases vector, of shape (1, N3)
 *       - W4: 4th layer weights (parameters) matrix, of shape (N3, K)
 *       - b4: 4th layer biases vector, of shape (1, K)
 */
train_paramserv = function(matrix[double] X, matrix[double] y,
                 matrix[double] X_val, matrix[double] y_val,
                 int C, int Hin, int Win, int epochs, int workers,
                 string utype, string freq, int batch_size, string mode, double learning_rate,
                 int seed = -1, boolean he = FALSE)
    return (list[unknown] model_trained) {
  N = nrow(X) # Number of inputs
  K = ncol(y) # Number of target classes

  # Define model network constants
  Hf = 3  # convolution filter height
  Wf = 3  # convolution filter width
  conv_stride = 1
  pool_stride = 2
  pad = 0  # For same dimensions, (Hf - stride) / 2
  F1 = 64
  F2 = F1 * 2
  F3 = F2 * 2
  F4 = F3 * 2
  F5 = F4 * 2

  # Create different seeds for each layer, unless seed is -1
  lseed = list()
  for ( i in 1:23 ){
    if (seed == -1){
        lseed = rbind(lseed, -1)
    } else {
        lseed = rbind(lseed, seed+1)
    }
  }

  # Initialize convolution weights

  # First step
  [W1, b1] = conv2d::init(F1, C, Hf, Wf, seed = lseed[1])  # inputs: (N, C*Hin*Win)
  [W2, b2] = conv2d::init(F1, F1, Hf, Wf, seed = lseed[2])
  # Second step
  [W3, b3] = conv2d::init(F2, F1, Hf, Wf, seed = lseed[3])
  [W4, b4] = conv2d::init(F2, F2, Hf, Wf, seed = lseed[4])
  # Third step
  [W5, b5] = conv2d::init(F3, F2, Hf, Wf, seed = lseed[5])
  [W6, b6] = conv2d::init(F3, F3, Hf, Wf, seed = lseed[6])
  # Fourth step
  [W7, b7] = conv2d::init(F4, F3, Hf, Wf, seed = lseed[7])
  [W8, b8] = conv2d::init(F4, F4, Hf, Wf, seed = lseed[8])
  # Fifth step
  [W9, b9] = conv2d::init(F5, F4, Hf, Wf, seed = lseed[9])
  [W10, b10] = conv2d::init(F5, F5, Hf, Wf, seed = lseed[10])
  # First Up-convolution
  [W11, b11] = conv2d_transpose::init(F5, F5, Hf, Wf, seed = lseed[11])
  [W12, b12] = conv2d::init(F4, F5, Hf, Wf, seed = lseed[12])
  [W13, b13] = conv2d::init(F4, F4, Hf, Wf, seed = lseed[13])
  # Second Up-convolution
  [W14, b14] = conv2d_transpose::init(F4, F4, Hf, Wf, seed = lseed[14])
  [W15, b15] = conv2d::init(F3, F4, Hf, Wf, seed = lseed[15])
  [W16, b16] = conv2d::init(F3, F3, Hf, Wf, seed = lseed[16])
  # Third Up-convolution
  [W17, b17] = conv2d_transpose::init(F3, F3, Hf, Wf, seed = lseed[17])
  [W18, b18] = conv2d::init(F2, F3, Hf, Wf, seed = lseed[18])
  [W19, b19] = conv2d::init(F2, F2, Hf, Wf, seed = lseed[19])
  # Fourth Up-convolution
  [W20, b20] = conv2d_transpose::init(F2, F2, Hf, Wf, seed = lseed[20])
  [W21, b21] = conv2d::init(F1, F2, Hf, Wf, seed = lseed[21])
  [W22, b22] = conv2d::init(F1, F1, Hf, Wf, seed = lseed[22])
  # Segmentation map (with K target classes)
  [W23, b23] = conv2d::init(K, F1, Hf, Wf, seed = lseed[23]) # Output: (N,K)

  # Initialize SGD with momentum
  vW1 = sgd_momentum::init(W1); vb1 = sgd_momentum::init(b1)
  vW2 = sgd_momentum::init(W2); vb2 = sgd_momentum::init(b2)
  vW3 = sgd_momentum::init(W3); vb3 = sgd_momentum::init(b3)
  vW4 = sgd_momentum::init(W4); vb4 = sgd_momentum::init(b4)
  vW5 = sgd_momentum::init(W5); vb5 = sgd_momentum::init(b5)
  vW6 = sgd_momentum::init(W6); vb6 = sgd_momentum::init(b6)
  vW7 = sgd_momentum::init(W7); vb7 = sgd_momentum::init(b7)
  vW8 = sgd_momentum::init(W8); vb8 = sgd_momentum::init(b8)
  vW9 = sgd_momentum::init(W9); vb9 = sgd_momentum::init(b9)
  vW10 = sgd_momentum::init(W10); vb10 = sgd_momentum::init(b10)
  vW11 = sgd_momentum::init(W11); vb11 = sgd_momentum::init(b11)
  vW12 = sgd_momentum::init(W12); vb12 = sgd_momentum::init(b12)
  vW13 = sgd_momentum::init(W13); vb13 = sgd_momentum::init(b13)
  vW14 = sgd_momentum::init(W14); vb14 = sgd_momentum::init(b14)
  vW15 = sgd_momentum::init(W15); vb15 = sgd_momentum::init(b15)
  vW16 = sgd_momentum::init(W16); vb16 = sgd_momentum::init(b16)
  vW17 = sgd_momentum::init(W17); vb17 = sgd_momentum::init(b17)
  vW18 = sgd_momentum::init(W18); vb18 = sgd_momentum::init(b18)
  vW19 = sgd_momentum::init(W19); vb19 = sgd_momentum::init(b19)
  vW20 = sgd_momentum::init(W20); vb20 = sgd_momentum::init(b20)
  vW21 = sgd_momentum::init(W21); vb21 = sgd_momentum::init(b21)
  vW22 = sgd_momentum::init(W22); vb22 = sgd_momentum::init(b22)
  vW23 = sgd_momentum::init(W23); vb23 = sgd_momentum::init(b23)

  # Define optimizer constants
  learning_rate = learning_rate  # learning rate
  mu = 0.9  # momentum
  decay = 0.95  # learning rate decay constant

  # Regularization
  lambda = 5e-04

  # Create the model list
  model_list = list(
    W1, W2, W3, W4, W5, W6, W7, W8, W9, W10, W11, W12, W13, W14, W15, W16, W17, W18, W19, W20, W21, W22, W23,
    b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15, b16, b17, b18, b19, b20, b21, b22, b23,
    vW1, vW2, vW3, vW4, vW5, vW6, vW7, vW8, vW9, vW10, vW11, vW12, vW13, vW14, vW15, vW16, vW17, vW18, vW19, vW20, vW21, vW22, vW23,
    vb1, vb2, vb3, vb4, vb5, vb6, vb7, vb8, vb9, vb10, vb11, vb12, vb13, vb14, vb15, vb16, vb17, vb18, vb19, vb20, vb21, vb22, vb23)

  # Create the hyper parameter list
  params = list(
    learning_rate=learning_rate, mu=mu, decay=decay, C=C, Hin=Hin, Win=Win, Hf=Hf, Wf=Wf, stride=stride, pad=pad, lambda=lambda,
    F1=F1, F2=F2, F3=F3, F4=F4, F5=F5)

  # Use paramserv function
  model_trained = paramserv(model=model_list, features=X, labels=y, val_features=X_val, val_labels=y_val,
    upd="./UNet.dml::gradients", agg="./UNet.dml::aggregation",
    mode=mode, utype=utype, freq=freq, epochs=epochs, batchsize=batch_size,
    k=workers, hyperparams=params, checkpointing="NONE", he=he, modelAvg=TRUE)
}

predict = function(matrix[double] X, int C, int Hin, int Win, int batch_size, list[unknown] model)
    return (matrix[double] probs) {
  W1 = as.matrix(model[1])
  W2 = as.matrix(model[2])
  W3 = as.matrix(model[3])
  W4 = as.matrix(model[4])
  W5 = as.matrix(model[5])
  W6 = as.matrix(model[6])
  W7 = as.matrix(model[7])
  W8 = as.matrix(model[8])
  W9 = as.matrix(model[9])
  W10 = as.matrix(model[10])
  W11 = as.matrix(model[11])
  W12 = as.matrix(model[12])
  W13 = as.matrix(model[13])
  W14 = as.matrix(model[14])
  W15 = as.matrix(model[15])
  W16 = as.matrix(model[16])
  W17 = as.matrix(model[17])
  W18 = as.matrix(model[18])
  W19 = as.matrix(model[19])
  W20 = as.matrix(model[20])
  W21 = as.matrix(model[21])
  W22 = as.matrix(model[22])
  W23 = as.matrix(model[23])
  b1 = as.matrix(model[24])
  b2 = as.matrix(model[25])
  b3 = as.matrix(model[26])
  b4 = as.matrix(model[27])
  b5 = as.matrix(model[28])
  b6 = as.matrix(model[29])
  b7 = as.matrix(model[30])
  b8 = as.matrix(model[31])
  b9 = as.matrix(model[32])
  b10 = as.matrix(model[33])
  b11 = as.matrix(model[34])
  b12 = as.matrix(model[35])
  b13 = as.matrix(model[36])
  b14 = as.matrix(model[37])
  b15 = as.matrix(model[38])
  b16 = as.matrix(model[39])
  b17 = as.matrix(model[40])
  b18 = as.matrix(model[41])
  b19 = as.matrix(model[42])
  b20 = as.matrix(model[43])
  b21 = as.matrix(model[44])
  b22 = as.matrix(model[45])
  b23 = as.matrix(model[46])
  N = nrow(X)

  Hf = 3  # convolution filter height
  Wf = 3  # convolution filter width
  conv_stride = 1
  pool_stride = 2
  pad = 0  # For same dimensions, (Hf - stride) / 2
  F1 = 64
  F2 = F1 * 2
  F3 = F2 * 2
  F4 = F3 * 2
  F5 = F4 * 2
  dropProb = 0.9
  dropSeed = -1

  # Compute predictions over mini-batches
  probs = matrix(0, rows=N, cols=K)
  iters = ceil(N / batch_size)
  for(i in 1:iters, check=0) {
    # Get next batch
    beg = ((i-1) * batch_size) %% N + 1
    end = min(N, beg + batch_size - 1)
    X_batch = X[beg:end,]

    # Compute forward pass

    # Down-Convolution
    [outc1, Houtc1, Woutc1] = conv2d::forward(X_batch, W1, b1, C, Hin, Win, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr1 = relu::forward(outc1)
    [outc2, Houtc2, Woutc2] = conv2d::forward(outr1, W2, b2, F1, Houtp1, Woutp1, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr2 = relu::forward(outc2)
    [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr2, F1, Houtc2, Woutc2, 2, 2, pool_stride, pool_stride, 0, 0)
    [outc3, Houtc3, Woutc3] = conv2d::forward(outp1, W3, b3, F2, Houtp1, Woutp1, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr3 = relu::forward(outc3)
    [outc4, Houtc4, Woutc4] = conv2d::forward(outr3, W4, b4, F2, Houtc3, Woutc3, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr4 = relu::forward(outc4)
    [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr4, F2, Houtc4, Woutc4, 2, 2, pool_stride, pool_stride, 0, 0)
    [outc5, Houtc5, Woutc5] = conv2d::forward(outp2, W5, b5, F3, Houtp2, Woutp2, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr5 = relu::forward(outc5)
    [outc6, Houtc6, Woutc6] = conv2d::forward(outr5, W6, b6, F3, Houtc5, Woutc5, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr6 = relu::forward(outc6)
    [outp3, Houtp3, Woutp3] = max_pool2d::forward(outr6, F3, Houtc6, Woutc6, 2, 2, pool_stride, pool_stride, 0, 0)
    [outc7, Houtc7, Woutc7] = conv2d::forward(outp3, W7, b7, F4, Houtp3, Woutp3, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr7 = relu::forward(outc7)
    [outc8, Houtc8, Woutc8] = conv2d::forward(outr7, W8, b8, F4, Houtc7, Woutc7, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr8 = relu::forward(outc8)
    [outp4, Houtp4, Woutp4] = max_pool2d::forward(outr8, F4, Houtc8, Woutc8, 2, 2, pool_stride, pool_stride, 0, 0)
    [outd1, mask1]          = dropout::forward(outp4, dropProb, dropSeed)

    # Bottom
    [outc9, Houtc9, Woutc9] = conv2d::forward(outd1, W9, b9, F5, Houtp4, Woutp4, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr9 = relu::forward(outc9)
    [outc10, Houtc10, Woutc10] = conv2d::forward(outr9, W10, b10, F5, Houtc9, Woutc9, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr10 = relu::forward(outc10)
    [outc11, Houtc11, Woutc11] = conv2d_transpose::forward(outr10, W11, b11, F5, Houtc10, Woutc10, Hf, Wf, conv_stride, conv_stride, pad, pad, 0, 0)

    # Up-Convolution
    outConcat1 = cbind(outc8,outc11)
    WoutConcat1 = Woutc11*2
    [outc12, Houtc12, Woutc12] = conv2d::forward(outConcat1, W12, b12, F4, Houtc11, WoutConcat1, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr11 = relu::forward(outc12)
    [outc13, Houtc13, Woutc13] = conv2d::forward(outr11, W13, b13, F4, Houtc12, Woutc12, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr12 = relu::forward(outc13)
    [outc14, Houtc14, Woutc14] = conv2d_transpose::forward(outr12, W14, b14, F4, Houtc13, Woutc13, Hf, Wf, conv_stride, conv_stride, pad, pad, 0, 0)
    outConcat2 = cbind(outc6,outc14)
    WoutConcat2 = Woutc14*2
    [outc15, Houtc15, Woutc15] = conv2d::forward(outConcat2, W15, b15, F3, Houtc14, WoutConcat2, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr13 = relu::forward(outc15)
    [outc16, Houtc16, Woutc16] = conv2d::forward(outr13, W16, b16, F3, Houtc15, Woutc15, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr14 = relu::forward(outc16)
    [outc17, Houtc17, Woutc17] = conv2d_transpose::forward(outr14, W17, b17, F3, Houtc16, Woutc16, Hf, Wf, conv_stride, conv_stride, pad, pad, 0, 0)
    outConcat3 = cbind(outc4, outc17)
    WoutConcat3 = Woutc17*2
    [outc18, Houtc18, Woutc18] = conv2d::forward(outConcat3, W18, b18, F2, Houtc17, WoutConcat3, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr15 = relu::forward(outc18)
    [outc19, Houtc19, Woutc19] = conv2d::forward(outr15, W19, b19, F2, Houtc18, Woutc18, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr16 = relu::forward(outc19)
    [outc20, Houtc20, Woutc20] = conv2d_transpose::forward(outr16, W20, b20, F3, Houtc19, Woutc19, Hf, Wf, conv_stride, conv_stride, pad, pad, 0, 0)
    outConcat4 = cbind(outc2, outc20)
    WoutConcat4 = Woutc20*2
    [outc21, Houtc21, Woutc21] = conv2d::forward(outConcat4, W21, b21, F1, Houtc20, WoutConcat4, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr17 = relu::forward(outc21)
    [outc22, Houtc22, Woutc22] = conv2d::forward(outr17, W22, b22, F1, Houtc21, Woutc21, Hf, Wf, conv_stride, conv_stride, pad, pad)
    outr18 = relu::forward(outc22)

    # This last conv2d needs to create the segmentation map (1x1 filter):
    [outc23, Houtc23, Woutc23] = conv2d::forward(outr18, W23, b23, F1, Houtc22, Woutc22, 1, 1, conv_stride, conv_stride, pad, pad)

    # Store predictions
    probs[beg:end,] = outc23
  }
}

gradients = function(list[unknown] model,
                     list[unknown] hyperparams,
                     matrix[double] features,
                     matrix[double] labels)
          return (list[unknown] gradients) {
        C = as.integer(as.scalar(hyperparams["C"]))
        Hin = as.integer(as.scalar(hyperparams["Hin"]))
        Win = as.integer(as.scalar(hyperparams["Win"]))
        Hf = as.integer(as.scalar(hyperparams["Hf"]))
        Wf = as.integer(as.scalar(hyperparams["Wf"]))
        stride = as.integer(as.scalar(hyperparams["stride"]))
        pad = as.integer(as.scalar(hyperparams["pad"]))
        lambda = as.double(as.scalar(hyperparams["lambda"]))
        F1 = as.integer(as.scalar(hyperparams["F1"]))
        F2 = as.integer(as.scalar(hyperparams["F2"]))
        N3 = as.integer(as.scalar(hyperparams["N3"]))
        W1 = as.matrix(model[1])
        W2 = as.matrix(model[2])
        W3 = as.matrix(model[3])
        W4 = as.matrix(model[4])
        b1 = as.matrix(model[5])
        b2 = as.matrix(model[6])
        b3 = as.matrix(model[7])
        b4 = as.matrix(model[8])

        # Compute forward pass
        ## layer 1: conv1 -> relu1 -> pool1
        [outc1, Houtc1, Woutc1] = conv2d::forward(features, W1, b1, C, Hin, Win, Hf, Wf,
                                                    stride, stride, pad, pad)
        outr1 = relu::forward(outc1)
        [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr1, F1, Houtc1, Woutc1, 2, 2, 2, 2, 0, 0)
        ## layer 2: conv2 -> relu2 -> pool2
        [outc2, Houtc2, Woutc2] = conv2d::forward(outp1, W2, b2, F1, Houtp1, Woutp1, Hf, Wf,
                                                  stride, stride, pad, pad)
        outr2 = relu::forward(outc2)
        [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr2, F2, Houtc2, Woutc2, 2, 2, 2, 2, 0, 0)
        ## layer 3:  affine3 -> relu3 -> dropout
        outa3 = affine::forward(outp2, W3, b3)
        outr3 = relu::forward(outa3)
        [outd3, maskd3] = dropout::forward(outr3, 0.5, -1)
        ## layer 4:  affine4 -> softmax
        outa4 = affine::forward(outd3, W4, b4)
        probs = softmax::forward(outa4)

        # Compute loss & accuracy for training data
        loss = cross_entropy_loss::forward(probs, labels)
        accuracy = mean(rowIndexMax(probs) == rowIndexMax(labels))
        # print("[+] Completed forward pass on batch: train loss: " + loss + ", train accuracy: " + accuracy)

        # Compute data backward pass
        ## loss
        dprobs = cross_entropy_loss::backward(probs, labels)
        ## layer 4:  affine4 -> softmax
        douta4 = softmax::backward(dprobs, outa4)
        [doutd3, dW4, db4] = affine::backward(douta4, outr3, W4, b4)
        ## layer 3:  affine3 -> relu3 -> dropout
        doutr3 = dropout::backward(doutd3, outr3, 0.5, maskd3)
        douta3 = relu::backward(doutr3, outa3)
        [doutp2, dW3, db3] = affine::backward(douta3, outp2, W3, b3)
        ## layer 2: conv2 -> relu2 -> pool2
        doutr2 = max_pool2d::backward(doutp2, Houtp2, Woutp2, outr2, F2, Houtc2, Woutc2, 2, 2, 2, 2, 0, 0)
        doutc2 = relu::backward(doutr2, outc2)
        [doutp1, dW2, db2] = conv2d::backward(doutc2, Houtc2, Woutc2, outp1, W2, b2, F1,
                                              Houtp1, Woutp1, Hf, Wf, stride, stride, pad, pad)
        ## layer 1: conv1 -> relu1 -> pool1
        doutr1 = max_pool2d::backward(doutp1, Houtp1, Woutp1, outr1, F1, Houtc1, Woutc1, 2, 2, 2, 2, 0, 0)
        doutc1 = relu::backward(doutr1, outc1)
        [dX_batch, dW1, db1] = conv2d::backward(doutc1, Houtc1, Woutc1, features, W1, b1, C, Hin, Win,
                                                Hf, Wf, stride, stride, pad, pad)

        # Compute regularization backward pass
        dW1_reg = l2_reg::backward(W1, lambda)
        dW2_reg = l2_reg::backward(W2, lambda)
        dW3_reg = l2_reg::backward(W3, lambda)
        dW4_reg = l2_reg::backward(W4, lambda)
        dW1 = dW1 + dW1_reg
        dW2 = dW2 + dW2_reg
        dW3 = dW3 + dW3_reg
        dW4 = dW4 + dW4_reg

        gradients = list(dW1, dW2, dW3, dW4, db1, db2, db3, db4)
    }

aggregation = function(list[unknown] model,
                       list[unknown] hyperparams,
                       list[unknown] gradients)
    return (list[unknown] model_result) {
    W1 = as.matrix(model[1])
       W2 = as.matrix(model[2])
       W3 = as.matrix(model[3])
       W4 = as.matrix(model[4])
       b1 = as.matrix(model[5])
       b2 = as.matrix(model[6])
       b3 = as.matrix(model[7])
       b4 = as.matrix(model[8])
       dW1 = as.matrix(gradients[1])
       dW2 = as.matrix(gradients[2])
       dW3 = as.matrix(gradients[3])
       dW4 = as.matrix(gradients[4])
       db1 = as.matrix(gradients[5])
       db2 = as.matrix(gradients[6])
       db3 = as.matrix(gradients[7])
       db4 = as.matrix(gradients[8])
       vW1 = as.matrix(model[9])
       vW2 = as.matrix(model[10])
       vW3 = as.matrix(model[11])
       vW4 = as.matrix(model[12])
       vb1 = as.matrix(model[13])
       vb2 = as.matrix(model[14])
       vb3 = as.matrix(model[15])
       vb4 = as.matrix(model[16])
       learning_rate = as.double(as.scalar(hyperparams["learning_rate"]))
       mu = as.double(as.scalar(hyperparams["mu"]))

       # Optimize with SGD w/ Nesterov momentum
       [W1, vW1] = sgd_nesterov::update(W1, dW1, learning_rate, mu, vW1)
       [b1, vb1] = sgd_nesterov::update(b1, db1, learning_rate, mu, vb1)
       [W2, vW2] = sgd_nesterov::update(W2, dW2, learning_rate, mu, vW2)
       [b2, vb2] = sgd_nesterov::update(b2, db2, learning_rate, mu, vb2)
       [W3, vW3] = sgd_nesterov::update(W3, dW3, learning_rate, mu, vW3)
       [b3, vb3] = sgd_nesterov::update(b3, db3, learning_rate, mu, vb3)
       [W4, vW4] = sgd_nesterov::update(W4, dW4, learning_rate, mu, vW4)
       [b4, vb4] = sgd_nesterov::update(b4, db4, learning_rate, mu, vb4)

       model_result = list(W1, W2, W3, W4, b1, b2, b3, b4, vW1, vW2, vW3, vW4, vb1, vb2, vb3, vb4)
}

/*
 * Gives the accuracy and loss for a model and given feature and label matrices
 *
 * This function is a combination of the predict and eval function used for validation.
 * For inputs see eval and predict.
 *
 * Outputs:
 *  - loss: Scalar loss, of shape (1).
 *  - accuracy: Scalar accuracy, of shape (1).
 */
validate = function(matrix[double] val_features, matrix[double] val_labels,
  list[unknown] model, list[unknown] hyperparams)
	return (double loss, double accuracy)
{
  [loss, accuracy] = eval(predict(val_features, as.integer(as.scalar(hyperparams["C"])),
    as.integer(as.scalar(hyperparams["Hin"])), as.integer(as.scalar(hyperparams["Win"])),
    32, model), val_labels)
}
